{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7ddb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ComputationalGraphPrimer import *\n",
    "import operator\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541f9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDPlus(ComputationalGraphPrimer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def run_training_loop_multi_neuron_model(self, training_data,mu=0.0,SGDplus=False):\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how \n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "           \n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of \n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "        \n",
    "        ##My input start\n",
    "        self.bias_update = [0.0]*(self.num_layers+1)\n",
    "        self.step = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1)\n",
    "        self.mu = mu if SGDplus else 0.0      \n",
    "        \n",
    "        ##My input end\n",
    "        \n",
    "        \n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        for i in range(self.training_iterations):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            self.backprop_and_update_params_multi_neuron_model(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "        return loss_running_record\n",
    "           \n",
    "\n",
    "\n",
    "    def forward_prop_multi_neuron_model(self, data_tuples_in_batch):\n",
    "        \"\"\"\n",
    "        During forward propagation, we push each batch of the input data through the\n",
    "        network.  In order to explain the logic of forward, consider the following network\n",
    "        layout in 4 nodes in the input layer, 2 nodes in the hidden layer, and 1 node in\n",
    "        the output layer.\n",
    "\n",
    "                               input\n",
    "                                  \n",
    "                                 x                                             x = node\n",
    "                                                                            \n",
    "                                 x         x|                                  | = sigmoid activation\n",
    "                                                     x|\n",
    "                                 x         x|   \n",
    "\n",
    "                                 x\n",
    "                            \n",
    "                             layer_0    layer_1    layer_2\n",
    "\n",
    "                \n",
    "        In the code shown below, the expressions to evaluate for computing the\n",
    "        pre-activation values at a node are stored at the layer in which the nodes reside.\n",
    "        That is, the dictionary look-up \"self.layer_exp_objects[layer_index]\" returns the\n",
    "        Expression objects for which the left-side dependent variable is in the layer\n",
    "        pointed to layer_index.  So the example shown above, \"self.layer_exp_objects[1]\"\n",
    "        will return two Expression objects, one for each of the two nodes in the second\n",
    "        layer of the network (that is, layer indexed 1).\n",
    "\n",
    "        The pre-activation values obtained by evaluating the expressions at each node are\n",
    "        then subject to Sigmoid activation, followed by the calculation of the partial\n",
    "        derivative of the output of the Sigmoid function with respect to its input.\n",
    "\n",
    "        In the forward, the values calculated for the nodes in each layer are stored in\n",
    "        the dictionary\n",
    "\n",
    "                        self.forw_prop_vals_at_layers[ layer_index ]\n",
    "\n",
    "        and the gradients values calculated at the same nodes in the dictionary:\n",
    "\n",
    "                        self.gradient_vals_for_layers[ layer_index ]\n",
    "\n",
    "        \"\"\"\n",
    "        self.forw_prop_vals_at_layers = {i : [] for i in range(self.num_layers)}   \n",
    "        self.gradient_vals_for_layers = {i : [] for i in range(1, self.num_layers)}\n",
    "        for vals_for_input_vars in data_tuples_in_batch:\n",
    "            self.forw_prop_vals_at_layers[0].append(vals_for_input_vars)\n",
    "            for layer_index in range(1, self.num_layers):\n",
    "                input_vars = self.layer_vars[layer_index-1]\n",
    "                if layer_index == 1:\n",
    "                    vals_for_input_vars_dict =  dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "                output_vals_arr = []\n",
    "                gradients_val_arr = []\n",
    "                ##  In the following loop for forward propagation calculations, exp_obj is the Exp object\n",
    "                ##  that is created for each user-supplied expression that specifies the network.  See the\n",
    "                ##  definition of the class Exp (for 'Expression') by searching for \"class Exp\":\n",
    "                for exp_obj in self.layer_exp_objects[layer_index]:\n",
    "                    output_val = self.eval_expression(exp_obj.body , vals_for_input_vars_dict,    \n",
    "                                                                 self.vals_for_learnable_params, input_vars)\n",
    "                    ## [Search for \"self.bias\" in this file.]  As mentioned earlier, adding bias to each \n",
    "                    ##  layer improves class discrimination:\n",
    "                    output_val = output_val + self.bias[layer_index-1]                \n",
    "                    ## apply sigmoid activation:\n",
    "                    output_val = 1.0 / (1.0 + np.exp(-1.0 * output_val))\n",
    "                    output_vals_arr.append(output_val)\n",
    "                    ## calculate partial of the activation function as a function of its input\n",
    "                    deriv_sigmoid = output_val * (1.0 - output_val)\n",
    "                    gradients_val_arr.append(deriv_sigmoid)\n",
    "                    vals_for_input_vars_dict[ exp_obj.dependent_var ] = output_val\n",
    "                self.forw_prop_vals_at_layers[layer_index].append(output_vals_arr)\n",
    "                ##  See the bullets in red on Slides 70 and 73 of my Week 3 slides for what needs\n",
    "                ##  to be stored during the forward propagation of data through the network:\n",
    "                self.gradient_vals_for_layers[layer_index].append(gradients_val_arr)\n",
    "\n",
    "\n",
    "    def backprop_and_update_params_multi_neuron_model(self, y_error, class_labels):\n",
    "        \"\"\"\n",
    "        First note that loop index variable 'back_layer_index' starts with the index of\n",
    "        the last layer.  For the 3-layer example shown for 'forward', back_layer_index\n",
    "        starts with a value of 2, its next value is 1, and that's it.\n",
    "\n",
    "        Stochastic Gradient Gradient calls for the backpropagated loss to be averaged over\n",
    "        the samples in a batch.  To explain how this averaging is carried out by the\n",
    "        backprop function, consider the last node on the example shown in the forward()\n",
    "        function above.  Standing at the node, we look at the 'input' values stored in the\n",
    "        variable \"input_vals\".  Assuming a batch size of 8, this will be list of\n",
    "        lists. Each of the inner lists will have two values for the two nodes in the\n",
    "        hidden layer. And there will be 8 of these for the 8 elements of the batch.  We average\n",
    "        these values 'input vals' and store those in the variable \"input_vals_avg\".  Next we\n",
    "        must carry out the same batch-based averaging for the partial derivatives stored in the\n",
    "        variable \"deriv_sigmoid\".\n",
    "\n",
    "        Pay attention to the variable 'vars_in_layer'.  These store the node variables in\n",
    "        the current layer during backpropagation.  Since back_layer_index starts with a\n",
    "        value of 2, the variable 'vars_in_layer' will have just the single node for the\n",
    "        example shown for forward(). With respect to what is stored in vars_in_layer', the\n",
    "        variables stored in 'input_vars_to_layer' correspond to the input layer with\n",
    "        respect to the current layer. \n",
    "        \"\"\"\n",
    "        # backproped prediction error:\n",
    "        pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "        pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "        for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "            input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "            input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "            input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "            deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "            deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "            deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                             [float(len(class_labels))] * len(class_labels)))\n",
    "            vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "            vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "            layer_params = self.layer_params[back_layer_index]         \n",
    "            ## note that layer_params are stored in a dict like        \n",
    "                ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "            ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "            transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "            backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "            for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                for j,var2 in enumerate(vars_in_layer):\n",
    "                    backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                               pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                               for i in range(len(vars_in_layer))])\n",
    "#                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "            pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "            input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "            for j,var in enumerate(vars_in_layer):\n",
    "                layer_params = self.layer_params[back_layer_index][j]\n",
    "                ##  Regarding the parameter update loop that follows, see the Slides 74 through 77 of my Week 3 \n",
    "                ##  lecture slides for how the parameters are updated using the partial derivatives stored away \n",
    "                ##  during forward propagation of data. The theory underlying these calculations is presented \n",
    "                ##  in Slides 68 through 71. \n",
    "                for i,param in enumerate(layer_params):\n",
    "                    gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j]\n",
    "                    \n",
    "                    #My change start\n",
    "                    self.step[back_layer_index-1][i] = (self.mu*self.step[back_layer_index-1][i]) +  gradient_of_loss_for_param * deriv_sigmoid_avg[j]\n",
    "                       \n",
    "                    \n",
    "                    self.vals_for_learnable_params[param] += self.step[back_layer_index-1][i]*self.learning_rate\n",
    "                    \n",
    "                    \n",
    "            self.bias_update[back_layer_index-1] = (self.mu*self.bias_update[back_layer_index-1]) + sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                           * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)       \n",
    "            self.bias[back_layer_index-1] += self.learning_rate * self.bias_update[back_layer_index-1]\n",
    "            \n",
    "            ##My change end\n",
    "    ######################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbabdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(ComputationalGraphPrimer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def run_training_loop_multi_neuron_model(self, training_data,beta1,beta2):\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how \n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "           \n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of \n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "        \n",
    "        ##My input start\n",
    "        self.bias_m = [0.0]*(self.num_layers+1)\n",
    "        self.bias_v = [0.0]*(self.num_layers+1)\n",
    "        self.bias_mh = [0.0]*(self.num_layers+1)\n",
    "        self.bias_vh = [0.0]*(self.num_layers+1)\n",
    "        \n",
    "        self.step_m = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1)\n",
    "        self.step_v = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1) \n",
    "        self.step_mh = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1)\n",
    "        self.step_vh = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1) \n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2  \n",
    "        self.m = 0\n",
    "        \n",
    "        ##My input end\n",
    "        \n",
    "        \n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        for i in range(self.training_iterations):\n",
    "            self.m = i+1\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            self.backprop_and_update_params_multi_neuron_model(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "        return loss_running_record\n",
    "           \n",
    "\n",
    "\n",
    "    def forward_prop_multi_neuron_model(self, data_tuples_in_batch):\n",
    "        \"\"\"\n",
    "        During forward propagation, we push each batch of the input data through the\n",
    "        network.  In order to explain the logic of forward, consider the following network\n",
    "        layout in 4 nodes in the input layer, 2 nodes in the hidden layer, and 1 node in\n",
    "        the output layer.\n",
    "\n",
    "                               input\n",
    "                                  \n",
    "                                 x                                             x = node\n",
    "                                                                            \n",
    "                                 x         x|                                  | = sigmoid activation\n",
    "                                                     x|\n",
    "                                 x         x|   \n",
    "\n",
    "                                 x\n",
    "                            \n",
    "                             layer_0    layer_1    layer_2\n",
    "\n",
    "                \n",
    "        In the code shown below, the expressions to evaluate for computing the\n",
    "        pre-activation values at a node are stored at the layer in which the nodes reside.\n",
    "        That is, the dictionary look-up \"self.layer_exp_objects[layer_index]\" returns the\n",
    "        Expression objects for which the left-side dependent variable is in the layer\n",
    "        pointed to layer_index.  So the example shown above, \"self.layer_exp_objects[1]\"\n",
    "        will return two Expression objects, one for each of the two nodes in the second\n",
    "        layer of the network (that is, layer indexed 1).\n",
    "\n",
    "        The pre-activation values obtained by evaluating the expressions at each node are\n",
    "        then subject to Sigmoid activation, followed by the calculation of the partial\n",
    "        derivative of the output of the Sigmoid function with respect to its input.\n",
    "\n",
    "        In the forward, the values calculated for the nodes in each layer are stored in\n",
    "        the dictionary\n",
    "\n",
    "                        self.forw_prop_vals_at_layers[ layer_index ]\n",
    "\n",
    "        and the gradients values calculated at the same nodes in the dictionary:\n",
    "\n",
    "                        self.gradient_vals_for_layers[ layer_index ]\n",
    "\n",
    "        \"\"\"\n",
    "        self.forw_prop_vals_at_layers = {i : [] for i in range(self.num_layers)}   \n",
    "        self.gradient_vals_for_layers = {i : [] for i in range(1, self.num_layers)}\n",
    "        for vals_for_input_vars in data_tuples_in_batch:\n",
    "            self.forw_prop_vals_at_layers[0].append(vals_for_input_vars)\n",
    "            for layer_index in range(1, self.num_layers):\n",
    "                input_vars = self.layer_vars[layer_index-1]\n",
    "                if layer_index == 1:\n",
    "                    vals_for_input_vars_dict =  dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "                output_vals_arr = []\n",
    "                gradients_val_arr = []\n",
    "                ##  In the following loop for forward propagation calculations, exp_obj is the Exp object\n",
    "                ##  that is created for each user-supplied expression that specifies the network.  See the\n",
    "                ##  definition of the class Exp (for 'Expression') by searching for \"class Exp\":\n",
    "                for exp_obj in self.layer_exp_objects[layer_index]:\n",
    "                    output_val = self.eval_expression(exp_obj.body , vals_for_input_vars_dict,    \n",
    "                                                                 self.vals_for_learnable_params, input_vars)\n",
    "                    ## [Search for \"self.bias\" in this file.]  As mentioned earlier, adding bias to each \n",
    "                    ##  layer improves class discrimination:\n",
    "                    output_val = output_val + self.bias[layer_index-1]                \n",
    "                    ## apply sigmoid activation:\n",
    "                    output_val = 1.0 / (1.0 + np.exp(-1.0 * output_val))\n",
    "                    output_vals_arr.append(output_val)\n",
    "                    ## calculate partial of the activation function as a function of its input\n",
    "                    deriv_sigmoid = output_val * (1.0 - output_val)\n",
    "                    gradients_val_arr.append(deriv_sigmoid)\n",
    "                    vals_for_input_vars_dict[ exp_obj.dependent_var ] = output_val\n",
    "                self.forw_prop_vals_at_layers[layer_index].append(output_vals_arr)\n",
    "                ##  See the bullets in red on Slides 70 and 73 of my Week 3 slides for what needs\n",
    "                ##  to be stored during the forward propagation of data through the network:\n",
    "                self.gradient_vals_for_layers[layer_index].append(gradients_val_arr)\n",
    "\n",
    "\n",
    "    def backprop_and_update_params_multi_neuron_model(self, y_error, class_labels):\n",
    "        \"\"\"\n",
    "        First note that loop index variable 'back_layer_index' starts with the index of\n",
    "        the last layer.  For the 3-layer example shown for 'forward', back_layer_index\n",
    "        starts with a value of 2, its next value is 1, and that's it.\n",
    "\n",
    "        Stochastic Gradient Gradient calls for the backpropagated loss to be averaged over\n",
    "        the samples in a batch.  To explain how this averaging is carried out by the\n",
    "        backprop function, consider the last node on the example shown in the forward()\n",
    "        function above.  Standing at the node, we look at the 'input' values stored in the\n",
    "        variable \"input_vals\".  Assuming a batch size of 8, this will be list of\n",
    "        lists. Each of the inner lists will have two values for the two nodes in the\n",
    "        hidden layer. And there will be 8 of these for the 8 elements of the batch.  We average\n",
    "        these values 'input vals' and store those in the variable \"input_vals_avg\".  Next we\n",
    "        must carry out the same batch-based averaging for the partial derivatives stored in the\n",
    "        variable \"deriv_sigmoid\".\n",
    "\n",
    "        Pay attention to the variable 'vars_in_layer'.  These store the node variables in\n",
    "        the current layer during backpropagation.  Since back_layer_index starts with a\n",
    "        value of 2, the variable 'vars_in_layer' will have just the single node for the\n",
    "        example shown for forward(). With respect to what is stored in vars_in_layer', the\n",
    "        variables stored in 'input_vars_to_layer' correspond to the input layer with\n",
    "        respect to the current layer. \n",
    "        \"\"\"\n",
    "        # backproped prediction error:\n",
    "        pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "        pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "        for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "            input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "            input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "            input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "            deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "            deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "            deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                             [float(len(class_labels))] * len(class_labels)))\n",
    "            vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "            vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "            layer_params = self.layer_params[back_layer_index]         \n",
    "            ## note that layer_params are stored in a dict like        \n",
    "                ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "            ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "            transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "            backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "            for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                for j,var2 in enumerate(vars_in_layer):\n",
    "                    backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                               pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                               for i in range(len(vars_in_layer))])\n",
    "#                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "            pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "            input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "            for j,var in enumerate(vars_in_layer):\n",
    "                layer_params = self.layer_params[back_layer_index][j]\n",
    "                ##  Regarding the parameter update loop that follows, see the Slides 74 through 77 of my Week 3 \n",
    "                ##  lecture slides for how the parameters are updated using the partial derivatives stored away \n",
    "                ##  during forward propagation of data. The theory underlying these calculations is presented \n",
    "                ##  in Slides 68 through 71. \n",
    "                for i,param in enumerate(layer_params):\n",
    "                    gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j]\n",
    "                    \n",
    "                    #My change start\n",
    "                    self.step_m[back_layer_index-1][i] = (self.beta1*self.step_m[back_layer_index-1][i]) + (1-self.beta1)*(gradient_of_loss_for_param * deriv_sigmoid_avg[j])\n",
    "                    self.step_mh[back_layer_index-1][i] = self.step_m[back_layer_index-1][i]/(1-(self.beta1**self.m))                    \n",
    "                    self.step_v[back_layer_index-1][i] = (self.beta2*self.step_v[back_layer_index-1][i]) + (1-self.beta2)*((gradient_of_loss_for_param * deriv_sigmoid_avg[j])**2)\n",
    "                    self.step_vh[back_layer_index-1][i] = self.step_v[back_layer_index-1][i]/(1-(self.beta2**self.m))                    \n",
    "                    self.vals_for_learnable_params[param] += self.learning_rate * (self.step_mh[back_layer_index-1][i]/(np.sqrt(self.step_vh[back_layer_index-1][i])+10**-6))\n",
    "                    \n",
    "                    \n",
    "            self.bias_m[back_layer_index-1] = (self.beta1*self.bias_m[back_layer_index-1]) + (1-self.beta1)*(sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                           * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg))\n",
    "            self.bias_mh[back_layer_index-1] = self.bias_m[back_layer_index-1]/(1-(self.beta1**self.m))\n",
    "            self.bias_v[back_layer_index-1] = (self.beta2*self.bias_v[back_layer_index-1]) + (1-self.beta2)*((sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                           * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg))**2) \n",
    "            self.bias_vh[back_layer_index-1] = self.bias_v[back_layer_index-1]/(1-(self.beta2**self.m))\n",
    "            self.bias[back_layer_index-1] += self.learning_rate * (self.bias_mh[back_layer_index-1]/(np.sqrt(self.bias_vh[back_layer_index-1])+10**-6)) \n",
    "            \n",
    "            ##My change end\n",
    "    ######################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08452462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xw', 'xz', 'xr', 'xp', 'xq', 'xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'bp', 'ar', 'bs', 'ap', 'as', 'br', 'bq', 'aq'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xq', 'xp', 'xs', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xw': set(), 'xz': set(), 'xr': {'xw', 'xz'}, 'xp': {'xw', 'xz'}, 'xq': {'xw', 'xz'}, 'xs': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xw', 'xz', 'xr', 'xp', 'xo', 'xq', 'xs'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'bp', 'ar', 'bs', 'ap', 'as', 'br', 'cp', 'bq', 'aq', 'cq'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xq', 'xp', 'xs', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xw': {'xo'}, 'xz': {'xo'}, 'xr': {'xw', 'xz'}, 'xp': {'xw', 'xz'}, 'xo': set(), 'xq': {'xw', 'xz'}, 'xs': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xq', 'xp', 'xs', 'xr'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001EF503BFE20>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001EF503BFCD0>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001EF5034A6D0>]}\n"
     ]
    }
   ],
   "source": [
    "cgp1 = SGDPlus(\n",
    "               num_layers = 3,\n",
    "               layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "               expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                              'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                              'xo=cp*xw+cq*xz'],\n",
    "               output_vars = ['xo'],\n",
    "               dataset_size = 5000,\n",
    "               learning_rate = 1e-2,\n",
    "#               learning_rate = 5 * 1e-2,\n",
    "               training_iterations = 40000,\n",
    "               batch_size = 8,\n",
    "               display_loss_how_often = 100,\n",
    "               debug = True,\n",
    "      )\n",
    "\n",
    "cgp1.parse_multi_layer_expressions()\n",
    "training_data1 = cgp1.gen_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd05260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xw', 'xz', 'xr', 'xp', 'xq', 'xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'bp', 'ar', 'bs', 'ap', 'as', 'br', 'bq', 'aq'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xq', 'xp', 'xs', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xw': set(), 'xz': set(), 'xr': {'xw', 'xz'}, 'xp': {'xw', 'xz'}, 'xq': {'xw', 'xz'}, 'xs': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xw', 'xz', 'xr', 'xp', 'xo', 'xq', 'xs'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'bp', 'ar', 'bs', 'ap', 'as', 'br', 'cp', 'bq', 'aq', 'cq'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xq', 'xp', 'xs', 'xr'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xw': {'xo'}, 'xz': {'xo'}, 'xr': {'xw', 'xz'}, 'xp': {'xw', 'xz'}, 'xo': set(), 'xq': {'xw', 'xz'}, 'xs': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xq', 'xp', 'xs', 'xr'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001EF503F8DC0>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001EF503F8DF0>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001EF50362850>]}\n"
     ]
    }
   ],
   "source": [
    "cgp2 = Adam(\n",
    "               num_layers = 3,\n",
    "               layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "               expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                              'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                              'xo=cp*xw+cq*xz'],\n",
    "               output_vars = ['xo'],\n",
    "               dataset_size = 5000,\n",
    "               learning_rate = 1e-2,\n",
    "#               learning_rate = 5 * 1e-2,\n",
    "               training_iterations = 40000,\n",
    "               batch_size = 8,\n",
    "               display_loss_how_often = 100,\n",
    "               debug = True,\n",
    "      )\n",
    "\n",
    "cgp2.parse_multi_layer_expressions()\n",
    "training_data2 = cgp2.gen_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ac1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=1]  loss = 0.0023\n",
      "[iter=101]  loss = 0.2523\n",
      "[iter=201]  loss = 0.2470\n",
      "[iter=301]  loss = 0.2456\n",
      "[iter=401]  loss = 0.2453\n",
      "[iter=501]  loss = 0.2448\n",
      "[iter=601]  loss = 0.2436\n",
      "[iter=701]  loss = 0.2433\n",
      "[iter=801]  loss = 0.2438\n",
      "[iter=901]  loss = 0.2430\n",
      "[iter=1001]  loss = 0.2412\n",
      "[iter=1101]  loss = 0.2427\n",
      "[iter=1201]  loss = 0.2412\n",
      "[iter=1301]  loss = 0.2404\n",
      "[iter=1401]  loss = 0.2397\n",
      "[iter=1501]  loss = 0.2404\n",
      "[iter=1601]  loss = 0.2395\n",
      "[iter=1701]  loss = 0.2384\n",
      "[iter=1801]  loss = 0.2372\n",
      "[iter=1901]  loss = 0.2374\n",
      "[iter=2001]  loss = 0.2359\n",
      "[iter=2101]  loss = 0.2352\n",
      "[iter=2201]  loss = 0.2355\n",
      "[iter=2301]  loss = 0.2357\n",
      "[iter=2401]  loss = 0.2329\n",
      "[iter=2501]  loss = 0.2335\n",
      "[iter=2601]  loss = 0.2286\n",
      "[iter=2701]  loss = 0.2314\n",
      "[iter=2801]  loss = 0.2297\n",
      "[iter=2901]  loss = 0.2280\n",
      "[iter=3001]  loss = 0.2250\n",
      "[iter=3101]  loss = 0.2280\n",
      "[iter=3201]  loss = 0.2255\n",
      "[iter=3301]  loss = 0.2260\n",
      "[iter=3401]  loss = 0.2194\n",
      "[iter=3501]  loss = 0.2213\n",
      "[iter=3601]  loss = 0.2212\n",
      "[iter=3701]  loss = 0.2231\n",
      "[iter=3801]  loss = 0.2186\n",
      "[iter=3901]  loss = 0.2212\n",
      "[iter=4001]  loss = 0.2197\n",
      "[iter=4101]  loss = 0.2183\n",
      "[iter=4201]  loss = 0.2164\n",
      "[iter=4301]  loss = 0.2116\n",
      "[iter=4401]  loss = 0.2110\n",
      "[iter=4501]  loss = 0.2121\n",
      "[iter=4601]  loss = 0.2125\n",
      "[iter=4701]  loss = 0.2057\n",
      "[iter=4801]  loss = 0.2058\n",
      "[iter=4901]  loss = 0.2048\n",
      "[iter=5001]  loss = 0.2071\n",
      "[iter=5101]  loss = 0.2041\n",
      "[iter=5201]  loss = 0.2063\n",
      "[iter=5301]  loss = 0.1986\n",
      "[iter=5401]  loss = 0.2019\n",
      "[iter=5501]  loss = 0.2027\n",
      "[iter=5601]  loss = 0.2014\n",
      "[iter=5701]  loss = 0.1998\n",
      "[iter=5801]  loss = 0.1995\n",
      "[iter=5901]  loss = 0.2002\n",
      "[iter=6001]  loss = 0.1942\n",
      "[iter=6101]  loss = 0.1973\n",
      "[iter=6201]  loss = 0.1907\n",
      "[iter=6301]  loss = 0.1963\n",
      "[iter=6401]  loss = 0.1958\n",
      "[iter=6501]  loss = 0.1890\n",
      "[iter=6601]  loss = 0.1883\n",
      "[iter=6701]  loss = 0.1907\n",
      "[iter=6801]  loss = 0.1933\n",
      "[iter=6901]  loss = 0.1875\n",
      "[iter=7001]  loss = 0.1892\n",
      "[iter=7101]  loss = 0.1893\n",
      "[iter=7201]  loss = 0.1911\n",
      "[iter=7301]  loss = 0.1845\n",
      "[iter=7401]  loss = 0.1838\n",
      "[iter=7501]  loss = 0.1830\n",
      "[iter=7601]  loss = 0.1832\n",
      "[iter=7701]  loss = 0.1809\n",
      "[iter=7801]  loss = 0.1809\n",
      "[iter=7901]  loss = 0.1809\n",
      "[iter=8001]  loss = 0.1760\n",
      "[iter=8101]  loss = 0.1746\n",
      "[iter=8201]  loss = 0.1788\n",
      "[iter=8301]  loss = 0.1771\n",
      "[iter=8401]  loss = 0.1784\n",
      "[iter=8501]  loss = 0.1845\n",
      "[iter=8601]  loss = 0.1752\n",
      "[iter=8701]  loss = 0.1762\n",
      "[iter=8801]  loss = 0.1705\n",
      "[iter=8901]  loss = 0.1714\n",
      "[iter=9001]  loss = 0.1697\n",
      "[iter=9101]  loss = 0.1781\n",
      "[iter=9201]  loss = 0.1731\n",
      "[iter=9301]  loss = 0.1664\n",
      "[iter=9401]  loss = 0.1720\n",
      "[iter=9501]  loss = 0.1710\n",
      "[iter=9601]  loss = 0.1744\n",
      "[iter=9701]  loss = 0.1734\n",
      "[iter=9801]  loss = 0.1694\n",
      "[iter=9901]  loss = 0.1679\n",
      "[iter=10001]  loss = 0.1674\n",
      "[iter=10101]  loss = 0.1698\n",
      "[iter=10201]  loss = 0.1693\n",
      "[iter=10301]  loss = 0.1729\n",
      "[iter=10401]  loss = 0.1696\n",
      "[iter=10501]  loss = 0.1615\n",
      "[iter=10601]  loss = 0.1656\n",
      "[iter=10701]  loss = 0.1609\n",
      "[iter=10801]  loss = 0.1600\n",
      "[iter=10901]  loss = 0.1597\n",
      "[iter=11001]  loss = 0.1681\n",
      "[iter=11101]  loss = 0.1614\n",
      "[iter=11201]  loss = 0.1566\n",
      "[iter=11301]  loss = 0.1587\n",
      "[iter=11401]  loss = 0.1591\n",
      "[iter=11501]  loss = 0.1573\n",
      "[iter=11601]  loss = 0.1526\n",
      "[iter=11701]  loss = 0.1514\n",
      "[iter=11801]  loss = 0.1484\n",
      "[iter=11901]  loss = 0.1570\n",
      "[iter=12001]  loss = 0.1642\n",
      "[iter=12101]  loss = 0.1591\n",
      "[iter=12201]  loss = 0.1584\n",
      "[iter=12301]  loss = 0.1533\n",
      "[iter=12401]  loss = 0.1584\n",
      "[iter=12501]  loss = 0.1597\n",
      "[iter=12601]  loss = 0.1501\n",
      "[iter=12701]  loss = 0.1568\n",
      "[iter=12801]  loss = 0.1586\n",
      "[iter=12901]  loss = 0.1557\n",
      "[iter=13001]  loss = 0.1532\n",
      "[iter=13101]  loss = 0.1580\n",
      "[iter=13201]  loss = 0.1527\n",
      "[iter=13301]  loss = 0.1539\n",
      "[iter=13401]  loss = 0.1538\n",
      "[iter=13501]  loss = 0.1491\n",
      "[iter=13601]  loss = 0.1536\n",
      "[iter=13701]  loss = 0.1577\n",
      "[iter=13801]  loss = 0.1518\n",
      "[iter=13901]  loss = 0.1538\n",
      "[iter=14001]  loss = 0.1565\n",
      "[iter=14101]  loss = 0.1527\n",
      "[iter=14201]  loss = 0.1509\n",
      "[iter=14301]  loss = 0.1511\n",
      "[iter=14401]  loss = 0.1487\n",
      "[iter=14501]  loss = 0.1583\n",
      "[iter=14601]  loss = 0.1492\n",
      "[iter=14701]  loss = 0.1489\n",
      "[iter=14801]  loss = 0.1501\n",
      "[iter=14901]  loss = 0.1501\n",
      "[iter=15001]  loss = 0.1547\n",
      "[iter=15101]  loss = 0.1489\n",
      "[iter=15201]  loss = 0.1440\n",
      "[iter=15301]  loss = 0.1535\n",
      "[iter=15401]  loss = 0.1465\n",
      "[iter=15501]  loss = 0.1475\n",
      "[iter=15601]  loss = 0.1519\n",
      "[iter=15701]  loss = 0.1416\n",
      "[iter=15801]  loss = 0.1483\n",
      "[iter=15901]  loss = 0.1407\n",
      "[iter=16001]  loss = 0.1456\n",
      "[iter=16101]  loss = 0.1420\n",
      "[iter=16201]  loss = 0.1438\n",
      "[iter=16301]  loss = 0.1461\n",
      "[iter=16401]  loss = 0.1358\n",
      "[iter=16501]  loss = 0.1404\n",
      "[iter=16601]  loss = 0.1496\n",
      "[iter=16701]  loss = 0.1389\n",
      "[iter=16801]  loss = 0.1355\n",
      "[iter=16901]  loss = 0.1414\n",
      "[iter=17001]  loss = 0.1392\n",
      "[iter=17101]  loss = 0.1401\n",
      "[iter=17201]  loss = 0.1440\n",
      "[iter=17301]  loss = 0.1486\n",
      "[iter=17401]  loss = 0.1474\n",
      "[iter=17501]  loss = 0.1441\n",
      "[iter=17601]  loss = 0.1375\n",
      "[iter=17701]  loss = 0.1413\n",
      "[iter=17801]  loss = 0.1373\n",
      "[iter=17901]  loss = 0.1374\n",
      "[iter=18001]  loss = 0.1351\n",
      "[iter=18101]  loss = 0.1449\n",
      "[iter=18201]  loss = 0.1472\n",
      "[iter=18301]  loss = 0.1427\n",
      "[iter=18401]  loss = 0.1487\n",
      "[iter=18501]  loss = 0.1516\n",
      "[iter=18601]  loss = 0.1362\n",
      "[iter=18701]  loss = 0.1389\n",
      "[iter=18801]  loss = 0.1394\n",
      "[iter=18901]  loss = 0.1376\n",
      "[iter=19001]  loss = 0.1386\n",
      "[iter=19101]  loss = 0.1365\n",
      "[iter=19201]  loss = 0.1349\n",
      "[iter=19301]  loss = 0.1419\n",
      "[iter=19401]  loss = 0.1417\n",
      "[iter=19501]  loss = 0.1419\n",
      "[iter=19601]  loss = 0.1382\n",
      "[iter=19701]  loss = 0.1356\n",
      "[iter=19801]  loss = 0.1341\n",
      "[iter=19901]  loss = 0.1379\n",
      "[iter=20001]  loss = 0.1357\n",
      "[iter=20101]  loss = 0.1494\n",
      "[iter=20201]  loss = 0.1400\n",
      "[iter=20301]  loss = 0.1399\n",
      "[iter=20401]  loss = 0.1463\n",
      "[iter=20501]  loss = 0.1461\n",
      "[iter=20601]  loss = 0.1392\n",
      "[iter=20701]  loss = 0.1481\n",
      "[iter=20801]  loss = 0.1427\n",
      "[iter=20901]  loss = 0.1382\n",
      "[iter=21001]  loss = 0.1348\n",
      "[iter=21101]  loss = 0.1338\n",
      "[iter=21201]  loss = 0.1408\n",
      "[iter=21301]  loss = 0.1409\n",
      "[iter=21401]  loss = 0.1334\n",
      "[iter=21501]  loss = 0.1436\n",
      "[iter=21601]  loss = 0.1395\n",
      "[iter=21701]  loss = 0.1505\n",
      "[iter=21801]  loss = 0.1370\n",
      "[iter=21901]  loss = 0.1461\n",
      "[iter=22001]  loss = 0.1526\n",
      "[iter=22101]  loss = 0.1349\n",
      "[iter=22201]  loss = 0.1362\n",
      "[iter=22301]  loss = 0.1498\n",
      "[iter=22401]  loss = 0.1389\n",
      "[iter=22501]  loss = 0.1404\n",
      "[iter=22601]  loss = 0.1339\n",
      "[iter=22701]  loss = 0.1412\n",
      "[iter=22801]  loss = 0.1442\n",
      "[iter=22901]  loss = 0.1420\n",
      "[iter=23001]  loss = 0.1352\n",
      "[iter=23101]  loss = 0.1377\n",
      "[iter=23201]  loss = 0.1338\n",
      "[iter=23301]  loss = 0.1431\n",
      "[iter=23401]  loss = 0.1324\n",
      "[iter=23501]  loss = 0.1333\n",
      "[iter=23601]  loss = 0.1380\n",
      "[iter=23701]  loss = 0.1460\n",
      "[iter=23801]  loss = 0.1365\n",
      "[iter=23901]  loss = 0.1392\n",
      "[iter=24001]  loss = 0.1354\n",
      "[iter=24101]  loss = 0.1444\n",
      "[iter=24201]  loss = 0.1360\n",
      "[iter=24301]  loss = 0.1348\n",
      "[iter=24401]  loss = 0.1459\n",
      "[iter=24501]  loss = 0.1435\n",
      "[iter=24601]  loss = 0.1415\n",
      "[iter=24701]  loss = 0.1349\n",
      "[iter=24801]  loss = 0.1357\n",
      "[iter=24901]  loss = 0.1458\n",
      "[iter=25001]  loss = 0.1350\n",
      "[iter=25101]  loss = 0.1430\n",
      "[iter=25201]  loss = 0.1402\n",
      "[iter=25301]  loss = 0.1426\n",
      "[iter=25401]  loss = 0.1429\n",
      "[iter=25501]  loss = 0.1348\n",
      "[iter=25601]  loss = 0.1458\n",
      "[iter=25701]  loss = 0.1437\n",
      "[iter=25801]  loss = 0.1460\n",
      "[iter=25901]  loss = 0.1434\n",
      "[iter=26001]  loss = 0.1392\n",
      "[iter=26101]  loss = 0.1413\n",
      "[iter=26201]  loss = 0.1371\n",
      "[iter=26301]  loss = 0.1415\n",
      "[iter=26401]  loss = 0.1381\n",
      "[iter=26501]  loss = 0.1371\n",
      "[iter=26601]  loss = 0.1379\n",
      "[iter=26701]  loss = 0.1366\n",
      "[iter=26801]  loss = 0.1374\n",
      "[iter=26901]  loss = 0.1402\n",
      "[iter=27001]  loss = 0.1447\n",
      "[iter=27101]  loss = 0.1352\n",
      "[iter=27201]  loss = 0.1365\n",
      "[iter=27301]  loss = 0.1401\n",
      "[iter=27401]  loss = 0.1422\n",
      "[iter=27501]  loss = 0.1422\n",
      "[iter=27601]  loss = 0.1407\n",
      "[iter=27701]  loss = 0.1317\n",
      "[iter=27801]  loss = 0.1360\n",
      "[iter=27901]  loss = 0.1427\n",
      "[iter=28001]  loss = 0.1362\n",
      "[iter=28101]  loss = 0.1353\n",
      "[iter=28201]  loss = 0.1378\n",
      "[iter=28301]  loss = 0.1478\n",
      "[iter=28401]  loss = 0.1387\n",
      "[iter=28501]  loss = 0.1282\n",
      "[iter=28601]  loss = 0.1386\n",
      "[iter=28701]  loss = 0.1339\n",
      "[iter=28801]  loss = 0.1395\n",
      "[iter=28901]  loss = 0.1356\n",
      "[iter=29001]  loss = 0.1389\n",
      "[iter=29101]  loss = 0.1374\n",
      "[iter=29201]  loss = 0.1320\n",
      "[iter=29301]  loss = 0.1314\n",
      "[iter=29401]  loss = 0.1350\n",
      "[iter=29501]  loss = 0.1363\n",
      "[iter=29601]  loss = 0.1385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=29701]  loss = 0.1535\n",
      "[iter=29801]  loss = 0.1368\n",
      "[iter=29901]  loss = 0.1351\n",
      "[iter=30001]  loss = 0.1458\n",
      "[iter=30101]  loss = 0.1334\n",
      "[iter=30201]  loss = 0.1399\n",
      "[iter=30301]  loss = 0.1318\n",
      "[iter=30401]  loss = 0.1285\n",
      "[iter=30501]  loss = 0.1330\n",
      "[iter=30601]  loss = 0.1320\n",
      "[iter=30701]  loss = 0.1399\n",
      "[iter=30801]  loss = 0.1310\n",
      "[iter=30901]  loss = 0.1471\n",
      "[iter=31001]  loss = 0.1392\n",
      "[iter=31101]  loss = 0.1357\n",
      "[iter=31201]  loss = 0.1485\n",
      "[iter=31301]  loss = 0.1310\n",
      "[iter=31401]  loss = 0.1372\n",
      "[iter=31501]  loss = 0.1366\n",
      "[iter=31601]  loss = 0.1343\n",
      "[iter=31701]  loss = 0.1420\n",
      "[iter=31801]  loss = 0.1382\n",
      "[iter=31901]  loss = 0.1310\n",
      "[iter=32001]  loss = 0.1372\n",
      "[iter=32101]  loss = 0.1256\n",
      "[iter=32201]  loss = 0.1350\n",
      "[iter=32301]  loss = 0.1357\n",
      "[iter=32401]  loss = 0.1399\n",
      "[iter=32501]  loss = 0.1480\n",
      "[iter=32601]  loss = 0.1384\n",
      "[iter=32701]  loss = 0.1322\n",
      "[iter=32801]  loss = 0.1318\n",
      "[iter=32901]  loss = 0.1361\n",
      "[iter=33001]  loss = 0.1345\n",
      "[iter=33101]  loss = 0.1385\n",
      "[iter=33201]  loss = 0.1444\n",
      "[iter=33301]  loss = 0.1411\n",
      "[iter=33401]  loss = 0.1313\n",
      "[iter=33501]  loss = 0.1381\n",
      "[iter=33601]  loss = 0.1395\n",
      "[iter=33701]  loss = 0.1325\n",
      "[iter=33801]  loss = 0.1359\n",
      "[iter=33901]  loss = 0.1344\n",
      "[iter=34001]  loss = 0.1434\n",
      "[iter=34101]  loss = 0.1376\n",
      "[iter=34201]  loss = 0.1321\n",
      "[iter=34301]  loss = 0.1300\n",
      "[iter=34401]  loss = 0.1403\n",
      "[iter=34501]  loss = 0.1342\n",
      "[iter=34601]  loss = 0.1373\n",
      "[iter=34701]  loss = 0.1451\n",
      "[iter=34801]  loss = 0.1400\n",
      "[iter=34901]  loss = 0.1291\n",
      "[iter=35001]  loss = 0.1355\n",
      "[iter=35101]  loss = 0.1331\n",
      "[iter=35201]  loss = 0.1348\n",
      "[iter=35301]  loss = 0.1376\n",
      "[iter=35401]  loss = 0.1401\n",
      "[iter=35501]  loss = 0.1398\n",
      "[iter=35601]  loss = 0.1400\n",
      "[iter=35701]  loss = 0.1306\n",
      "[iter=35801]  loss = 0.1321\n",
      "[iter=35901]  loss = 0.1285\n",
      "[iter=36001]  loss = 0.1350\n",
      "[iter=36101]  loss = 0.1364\n",
      "[iter=36201]  loss = 0.1344\n",
      "[iter=36301]  loss = 0.1328\n",
      "[iter=36401]  loss = 0.1409\n",
      "[iter=36501]  loss = 0.1256\n",
      "[iter=36601]  loss = 0.1329\n",
      "[iter=36701]  loss = 0.1357\n",
      "[iter=36801]  loss = 0.1373\n",
      "[iter=36901]  loss = 0.1425\n",
      "[iter=37001]  loss = 0.1387\n",
      "[iter=37101]  loss = 0.1339\n",
      "[iter=37201]  loss = 0.1324\n",
      "[iter=37301]  loss = 0.1378\n",
      "[iter=37401]  loss = 0.1353\n",
      "[iter=37501]  loss = 0.1357\n",
      "[iter=37601]  loss = 0.1358\n",
      "[iter=37701]  loss = 0.1354\n",
      "[iter=37801]  loss = 0.1372\n",
      "[iter=37901]  loss = 0.1309\n",
      "[iter=38001]  loss = 0.1348\n",
      "[iter=38101]  loss = 0.1363\n",
      "[iter=38201]  loss = 0.1313\n",
      "[iter=38301]  loss = 0.1399\n",
      "[iter=38401]  loss = 0.1280\n",
      "[iter=38501]  loss = 0.1322\n",
      "[iter=38601]  loss = 0.1385\n",
      "[iter=38701]  loss = 0.1413\n",
      "[iter=38801]  loss = 0.1429\n",
      "[iter=38901]  loss = 0.1346\n",
      "[iter=39001]  loss = 0.1332\n",
      "[iter=39101]  loss = 0.1309\n",
      "[iter=39201]  loss = 0.1383\n",
      "[iter=39301]  loss = 0.1307\n",
      "[iter=39401]  loss = 0.1286\n",
      "[iter=39501]  loss = 0.1308\n",
      "[iter=39601]  loss = 0.1343\n",
      "[iter=39701]  loss = 0.1384\n",
      "[iter=39801]  loss = 0.1421\n",
      "[iter=39901]  loss = 0.1382\n",
      "[iter=1]  loss = 0.0029\n",
      "[iter=101]  loss = 0.2890\n",
      "[iter=201]  loss = 0.2744\n",
      "[iter=301]  loss = 0.2744\n",
      "[iter=401]  loss = 0.2744\n",
      "[iter=501]  loss = 0.2705\n",
      "[iter=601]  loss = 0.2672\n",
      "[iter=701]  loss = 0.2565\n",
      "[iter=801]  loss = 0.2535\n",
      "[iter=901]  loss = 0.2532\n",
      "[iter=1001]  loss = 0.2532\n",
      "[iter=1101]  loss = 0.2508\n",
      "[iter=1201]  loss = 0.2514\n",
      "[iter=1301]  loss = 0.2514\n",
      "[iter=1401]  loss = 0.2503\n",
      "[iter=1501]  loss = 0.2504\n",
      "[iter=1601]  loss = 0.2507\n",
      "[iter=1701]  loss = 0.2500\n",
      "[iter=1801]  loss = 0.2500\n",
      "[iter=1901]  loss = 0.2503\n",
      "[iter=2001]  loss = 0.2507\n",
      "[iter=2101]  loss = 0.2497\n",
      "[iter=2201]  loss = 0.2498\n",
      "[iter=2301]  loss = 0.2498\n",
      "[iter=2401]  loss = 0.2495\n",
      "[iter=2501]  loss = 0.2499\n",
      "[iter=2601]  loss = 0.2500\n",
      "[iter=2701]  loss = 0.2498\n",
      "[iter=2801]  loss = 0.2498\n",
      "[iter=2901]  loss = 0.2494\n",
      "[iter=3001]  loss = 0.2498\n",
      "[iter=3101]  loss = 0.2500\n",
      "[iter=3201]  loss = 0.2497\n",
      "[iter=3301]  loss = 0.2498\n",
      "[iter=3401]  loss = 0.2499\n",
      "[iter=3501]  loss = 0.2500\n",
      "[iter=3601]  loss = 0.2497\n",
      "[iter=3701]  loss = 0.2499\n",
      "[iter=3801]  loss = 0.2499\n",
      "[iter=3901]  loss = 0.2498\n",
      "[iter=4001]  loss = 0.2500\n",
      "[iter=4101]  loss = 0.2499\n",
      "[iter=4201]  loss = 0.2498\n",
      "[iter=4301]  loss = 0.2500\n",
      "[iter=4401]  loss = 0.2499\n",
      "[iter=4501]  loss = 0.2499\n",
      "[iter=4601]  loss = 0.2498\n",
      "[iter=4701]  loss = 0.2499\n",
      "[iter=4801]  loss = 0.2497\n",
      "[iter=4901]  loss = 0.2502\n",
      "[iter=5001]  loss = 0.2499\n",
      "[iter=5101]  loss = 0.2500\n",
      "[iter=5201]  loss = 0.2498\n",
      "[iter=5301]  loss = 0.2499\n",
      "[iter=5401]  loss = 0.2501\n",
      "[iter=5501]  loss = 0.2500\n",
      "[iter=5601]  loss = 0.2498\n",
      "[iter=5701]  loss = 0.2499\n",
      "[iter=5801]  loss = 0.2497\n",
      "[iter=5901]  loss = 0.2501\n",
      "[iter=6001]  loss = 0.2498\n",
      "[iter=6101]  loss = 0.2501\n",
      "[iter=6201]  loss = 0.2496\n",
      "[iter=6301]  loss = 0.2494\n",
      "[iter=6401]  loss = 0.2488\n",
      "[iter=6501]  loss = 0.2490\n",
      "[iter=6601]  loss = 0.2495\n",
      "[iter=6701]  loss = 0.2503\n",
      "[iter=6801]  loss = 0.2500\n",
      "[iter=6901]  loss = 0.2495\n",
      "[iter=7001]  loss = 0.2492\n",
      "[iter=7101]  loss = 0.2498\n",
      "[iter=7201]  loss = 0.2497\n",
      "[iter=7301]  loss = 0.2496\n",
      "[iter=7401]  loss = 0.2495\n",
      "[iter=7501]  loss = 0.2501\n",
      "[iter=7601]  loss = 0.2497\n",
      "[iter=7701]  loss = 0.2494\n",
      "[iter=7801]  loss = 0.2498\n",
      "[iter=7901]  loss = 0.2499\n",
      "[iter=8001]  loss = 0.2496\n",
      "[iter=8101]  loss = 0.2498\n",
      "[iter=8201]  loss = 0.2497\n",
      "[iter=8301]  loss = 0.2498\n",
      "[iter=8401]  loss = 0.2497\n",
      "[iter=8501]  loss = 0.2497\n",
      "[iter=8601]  loss = 0.2498\n",
      "[iter=8701]  loss = 0.2497\n",
      "[iter=8801]  loss = 0.2497\n",
      "[iter=8901]  loss = 0.2498\n",
      "[iter=9001]  loss = 0.2497\n",
      "[iter=9101]  loss = 0.2497\n",
      "[iter=9201]  loss = 0.2498\n",
      "[iter=9301]  loss = 0.2497\n",
      "[iter=9401]  loss = 0.2498\n",
      "[iter=9501]  loss = 0.2498\n",
      "[iter=9601]  loss = 0.2497\n",
      "[iter=9701]  loss = 0.2498\n",
      "[iter=9801]  loss = 0.2497\n",
      "[iter=9901]  loss = 0.2496\n",
      "[iter=10001]  loss = 0.2499\n",
      "[iter=10101]  loss = 0.2499\n",
      "[iter=10201]  loss = 0.2497\n",
      "[iter=10301]  loss = 0.2497\n",
      "[iter=10401]  loss = 0.2498\n",
      "[iter=10501]  loss = 0.2498\n",
      "[iter=10601]  loss = 0.2497\n",
      "[iter=10701]  loss = 0.2496\n",
      "[iter=10801]  loss = 0.2500\n",
      "[iter=10901]  loss = 0.2498\n",
      "[iter=11001]  loss = 0.2498\n",
      "[iter=11101]  loss = 0.2497\n",
      "[iter=11201]  loss = 0.2497\n",
      "[iter=11301]  loss = 0.2497\n",
      "[iter=11401]  loss = 0.2496\n",
      "[iter=11501]  loss = 0.2494\n",
      "[iter=11601]  loss = 0.2492\n",
      "[iter=11701]  loss = 0.2492\n",
      "[iter=11801]  loss = 0.2502\n",
      "[iter=11901]  loss = 0.2495\n",
      "[iter=12001]  loss = 0.2496\n",
      "[iter=12101]  loss = 0.2498\n",
      "[iter=12201]  loss = 0.2496\n",
      "[iter=12301]  loss = 0.2498\n",
      "[iter=12401]  loss = 0.2498\n",
      "[iter=12501]  loss = 0.2499\n",
      "[iter=12601]  loss = 0.2497\n",
      "[iter=12701]  loss = 0.2496\n",
      "[iter=12801]  loss = 0.2498\n",
      "[iter=12901]  loss = 0.2498\n",
      "[iter=13001]  loss = 0.2497\n",
      "[iter=13101]  loss = 0.2497\n",
      "[iter=13201]  loss = 0.2497\n",
      "[iter=13301]  loss = 0.2497\n",
      "[iter=13401]  loss = 0.2497\n",
      "[iter=13501]  loss = 0.2499\n",
      "[iter=13601]  loss = 0.2497\n",
      "[iter=13701]  loss = 0.2493\n",
      "[iter=13801]  loss = 0.2498\n",
      "[iter=13901]  loss = 0.2498\n",
      "[iter=14001]  loss = 0.2498\n",
      "[iter=14101]  loss = 0.2487\n",
      "[iter=14201]  loss = 0.2497\n",
      "[iter=14301]  loss = 0.2499\n",
      "[iter=14401]  loss = 0.2490\n",
      "[iter=14501]  loss = 0.2503\n",
      "[iter=14601]  loss = 0.2502\n",
      "[iter=14701]  loss = 0.2500\n",
      "[iter=14801]  loss = 0.2501\n",
      "[iter=14901]  loss = 0.2497\n",
      "[iter=15001]  loss = 0.2497\n",
      "[iter=15101]  loss = 0.2496\n",
      "[iter=15201]  loss = 0.2498\n",
      "[iter=15301]  loss = 0.2496\n",
      "[iter=15401]  loss = 0.2498\n",
      "[iter=15501]  loss = 0.2496\n",
      "[iter=15601]  loss = 0.2497\n",
      "[iter=15701]  loss = 0.2497\n",
      "[iter=15801]  loss = 0.2497\n",
      "[iter=15901]  loss = 0.2494\n",
      "[iter=16001]  loss = 0.2496\n",
      "[iter=16101]  loss = 0.2496\n",
      "[iter=16201]  loss = 0.2496\n",
      "[iter=16301]  loss = 0.2497\n",
      "[iter=16401]  loss = 0.2496\n",
      "[iter=16501]  loss = 0.2496\n",
      "[iter=16601]  loss = 0.2497\n",
      "[iter=16701]  loss = 0.2494\n",
      "[iter=16801]  loss = 0.2498\n",
      "[iter=16901]  loss = 0.2498\n",
      "[iter=17001]  loss = 0.2498\n",
      "[iter=17101]  loss = 0.2497\n",
      "[iter=17201]  loss = 0.2496\n",
      "[iter=17301]  loss = 0.2497\n",
      "[iter=17401]  loss = 0.2497\n",
      "[iter=17501]  loss = 0.2495\n",
      "[iter=17601]  loss = 0.2496\n",
      "[iter=17701]  loss = 0.2499\n",
      "[iter=17801]  loss = 0.2495\n",
      "[iter=17901]  loss = 0.2496\n",
      "[iter=18001]  loss = 0.2496\n",
      "[iter=18101]  loss = 0.2494\n",
      "[iter=18201]  loss = 0.2497\n",
      "[iter=18301]  loss = 0.2496\n",
      "[iter=18401]  loss = 0.2496\n",
      "[iter=18501]  loss = 0.2495\n",
      "[iter=18601]  loss = 0.2496\n",
      "[iter=18701]  loss = 0.2496\n",
      "[iter=18801]  loss = 0.2495\n",
      "[iter=18901]  loss = 0.2496\n",
      "[iter=19001]  loss = 0.2495\n",
      "[iter=19101]  loss = 0.2495\n",
      "[iter=19201]  loss = 0.2496\n",
      "[iter=19301]  loss = 0.2495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=19401]  loss = 0.2492\n",
      "[iter=19501]  loss = 0.2494\n",
      "[iter=19601]  loss = 0.2496\n",
      "[iter=19701]  loss = 0.2494\n",
      "[iter=19801]  loss = 0.2495\n",
      "[iter=19901]  loss = 0.2494\n",
      "[iter=20001]  loss = 0.2496\n",
      "[iter=20101]  loss = 0.2494\n",
      "[iter=20201]  loss = 0.2495\n",
      "[iter=20301]  loss = 0.2494\n",
      "[iter=20401]  loss = 0.2494\n",
      "[iter=20501]  loss = 0.2493\n",
      "[iter=20601]  loss = 0.2495\n",
      "[iter=20701]  loss = 0.2492\n",
      "[iter=20801]  loss = 0.2496\n",
      "[iter=20901]  loss = 0.2491\n",
      "[iter=21001]  loss = 0.2497\n",
      "[iter=21101]  loss = 0.2495\n",
      "[iter=21201]  loss = 0.2495\n",
      "[iter=21301]  loss = 0.2495\n",
      "[iter=21401]  loss = 0.2495\n",
      "[iter=21501]  loss = 0.2493\n",
      "[iter=21601]  loss = 0.2496\n",
      "[iter=21701]  loss = 0.2495\n",
      "[iter=21801]  loss = 0.2495\n",
      "[iter=21901]  loss = 0.2495\n",
      "[iter=22001]  loss = 0.2495\n",
      "[iter=22101]  loss = 0.2495\n",
      "[iter=22201]  loss = 0.2496\n",
      "[iter=22301]  loss = 0.2496\n",
      "[iter=22401]  loss = 0.2495\n",
      "[iter=22501]  loss = 0.2494\n",
      "[iter=22601]  loss = 0.2494\n",
      "[iter=22701]  loss = 0.2493\n",
      "[iter=22801]  loss = 0.2494\n",
      "[iter=22901]  loss = 0.2496\n",
      "[iter=23001]  loss = 0.2495\n",
      "[iter=23101]  loss = 0.2494\n",
      "[iter=23201]  loss = 0.2493\n",
      "[iter=23301]  loss = 0.2496\n",
      "[iter=23401]  loss = 0.2494\n",
      "[iter=23501]  loss = 0.2494\n",
      "[iter=23601]  loss = 0.2495\n",
      "[iter=23701]  loss = 0.2499\n",
      "[iter=23801]  loss = 0.2497\n",
      "[iter=23901]  loss = 0.2497\n",
      "[iter=24001]  loss = 0.2496\n",
      "[iter=24101]  loss = 0.2494\n",
      "[iter=24201]  loss = 0.2495\n",
      "[iter=24301]  loss = 0.2494\n",
      "[iter=24401]  loss = 0.2495\n",
      "[iter=24501]  loss = 0.2495\n",
      "[iter=24601]  loss = 0.2494\n",
      "[iter=24701]  loss = 0.2495\n",
      "[iter=24801]  loss = 0.2496\n",
      "[iter=24901]  loss = 0.2494\n",
      "[iter=25001]  loss = 0.2495\n",
      "[iter=25101]  loss = 0.2494\n",
      "[iter=25201]  loss = 0.2494\n",
      "[iter=25301]  loss = 0.2494\n",
      "[iter=25401]  loss = 0.2495\n",
      "[iter=25501]  loss = 0.2495\n",
      "[iter=25601]  loss = 0.2495\n",
      "[iter=25701]  loss = 0.2495\n",
      "[iter=25801]  loss = 0.2494\n",
      "[iter=25901]  loss = 0.2495\n",
      "[iter=26001]  loss = 0.2494\n",
      "[iter=26101]  loss = 0.2494\n",
      "[iter=26201]  loss = 0.2494\n",
      "[iter=26301]  loss = 0.2495\n",
      "[iter=26401]  loss = 0.2493\n",
      "[iter=26501]  loss = 0.2493\n",
      "[iter=26601]  loss = 0.2492\n",
      "[iter=26701]  loss = 0.2494\n",
      "[iter=26801]  loss = 0.2494\n",
      "[iter=26901]  loss = 0.2494\n",
      "[iter=27001]  loss = 0.2494\n",
      "[iter=27101]  loss = 0.2493\n",
      "[iter=27201]  loss = 0.2495\n",
      "[iter=27301]  loss = 0.2494\n",
      "[iter=27401]  loss = 0.2495\n",
      "[iter=27501]  loss = 0.2494\n",
      "[iter=27601]  loss = 0.2494\n",
      "[iter=27701]  loss = 0.2494\n",
      "[iter=27801]  loss = 0.2494\n",
      "[iter=27901]  loss = 0.2494\n",
      "[iter=28001]  loss = 0.2493\n",
      "[iter=28101]  loss = 0.2494\n",
      "[iter=28201]  loss = 0.2495\n",
      "[iter=28301]  loss = 0.2494\n",
      "[iter=28401]  loss = 0.2494\n",
      "[iter=28501]  loss = 0.2494\n",
      "[iter=28601]  loss = 0.2494\n",
      "[iter=28701]  loss = 0.2494\n",
      "[iter=28801]  loss = 0.2493\n",
      "[iter=28901]  loss = 0.2493\n",
      "[iter=29001]  loss = 0.2494\n",
      "[iter=29101]  loss = 0.2494\n",
      "[iter=29201]  loss = 0.2494\n",
      "[iter=29301]  loss = 0.2492\n",
      "[iter=29401]  loss = 0.2491\n",
      "[iter=29501]  loss = 0.2494\n",
      "[iter=29601]  loss = 0.2492\n",
      "[iter=29701]  loss = 0.2493\n",
      "[iter=29801]  loss = 0.2492\n",
      "[iter=29901]  loss = 0.2493\n",
      "[iter=30001]  loss = 0.2491\n",
      "[iter=30101]  loss = 0.2493\n",
      "[iter=30201]  loss = 0.2493\n",
      "[iter=30301]  loss = 0.2489\n",
      "[iter=30401]  loss = 0.2493\n",
      "[iter=30501]  loss = 0.2494\n",
      "[iter=30601]  loss = 0.2493\n",
      "[iter=30701]  loss = 0.2493\n",
      "[iter=30801]  loss = 0.2493\n",
      "[iter=30901]  loss = 0.2493\n",
      "[iter=31001]  loss = 0.2493\n",
      "[iter=31101]  loss = 0.2492\n",
      "[iter=31201]  loss = 0.2491\n",
      "[iter=31301]  loss = 0.2494\n",
      "[iter=31401]  loss = 0.2494\n",
      "[iter=31501]  loss = 0.2493\n",
      "[iter=31601]  loss = 0.2494\n",
      "[iter=31701]  loss = 0.2493\n",
      "[iter=31801]  loss = 0.2493\n",
      "[iter=31901]  loss = 0.2493\n",
      "[iter=32001]  loss = 0.2493\n",
      "[iter=32101]  loss = 0.2492\n",
      "[iter=32201]  loss = 0.2492\n",
      "[iter=32301]  loss = 0.2494\n",
      "[iter=32401]  loss = 0.2493\n",
      "[iter=32501]  loss = 0.2493\n",
      "[iter=32601]  loss = 0.2493\n",
      "[iter=32701]  loss = 0.2491\n",
      "[iter=32801]  loss = 0.2493\n",
      "[iter=32901]  loss = 0.2492\n",
      "[iter=33001]  loss = 0.2493\n",
      "[iter=33101]  loss = 0.2492\n",
      "[iter=33201]  loss = 0.2495\n",
      "[iter=33301]  loss = 0.2493\n",
      "[iter=33401]  loss = 0.2493\n",
      "[iter=33501]  loss = 0.2493\n",
      "[iter=33601]  loss = 0.2493\n",
      "[iter=33701]  loss = 0.2492\n",
      "[iter=33801]  loss = 0.2494\n",
      "[iter=33901]  loss = 0.2492\n",
      "[iter=34001]  loss = 0.2493\n",
      "[iter=34101]  loss = 0.2492\n",
      "[iter=34201]  loss = 0.2492\n",
      "[iter=34301]  loss = 0.2495\n",
      "[iter=34401]  loss = 0.2491\n",
      "[iter=34501]  loss = 0.2493\n",
      "[iter=34601]  loss = 0.2494\n",
      "[iter=34701]  loss = 0.2492\n",
      "[iter=34801]  loss = 0.2496\n",
      "[iter=34901]  loss = 0.2494\n",
      "[iter=35001]  loss = 0.2493\n",
      "[iter=35101]  loss = 0.2492\n",
      "[iter=35201]  loss = 0.2490\n",
      "[iter=35301]  loss = 0.2489\n",
      "[iter=35401]  loss = 0.2492\n",
      "[iter=35501]  loss = 0.2492\n",
      "[iter=35601]  loss = 0.2494\n",
      "[iter=35701]  loss = 0.2493\n",
      "[iter=35801]  loss = 0.2492\n",
      "[iter=35901]  loss = 0.2491\n",
      "[iter=36001]  loss = 0.2490\n",
      "[iter=36101]  loss = 0.2493\n",
      "[iter=36201]  loss = 0.2491\n",
      "[iter=36301]  loss = 0.2492\n",
      "[iter=36401]  loss = 0.2491\n",
      "[iter=36501]  loss = 0.2491\n",
      "[iter=36601]  loss = 0.2492\n",
      "[iter=36701]  loss = 0.2490\n",
      "[iter=36801]  loss = 0.2488\n",
      "[iter=36901]  loss = 0.2492\n",
      "[iter=37001]  loss = 0.2492\n",
      "[iter=37101]  loss = 0.2491\n",
      "[iter=37201]  loss = 0.2492\n",
      "[iter=37301]  loss = 0.2492\n",
      "[iter=37401]  loss = 0.2491\n",
      "[iter=37501]  loss = 0.2487\n",
      "[iter=37601]  loss = 0.2492\n",
      "[iter=37701]  loss = 0.2489\n",
      "[iter=37801]  loss = 0.2492\n",
      "[iter=37901]  loss = 0.2487\n",
      "[iter=38001]  loss = 0.2487\n",
      "[iter=38101]  loss = 0.2492\n",
      "[iter=38201]  loss = 0.2491\n",
      "[iter=38301]  loss = 0.2493\n",
      "[iter=38401]  loss = 0.2492\n",
      "[iter=38501]  loss = 0.2491\n",
      "[iter=38601]  loss = 0.2491\n",
      "[iter=38701]  loss = 0.2494\n",
      "[iter=38801]  loss = 0.2491\n",
      "[iter=38901]  loss = 0.2494\n",
      "[iter=39001]  loss = 0.2492\n",
      "[iter=39101]  loss = 0.2491\n",
      "[iter=39201]  loss = 0.2491\n",
      "[iter=39301]  loss = 0.2493\n",
      "[iter=39401]  loss = 0.2492\n",
      "[iter=39501]  loss = 0.2491\n",
      "[iter=39601]  loss = 0.2491\n",
      "[iter=39701]  loss = 0.2490\n",
      "[iter=39801]  loss = 0.2491\n",
      "[iter=39901]  loss = 0.2492\n",
      "[iter=1]  loss = 0.0026\n",
      "[iter=101]  loss = 0.3001\n",
      "[iter=201]  loss = 0.2600\n",
      "[iter=301]  loss = 0.2473\n",
      "[iter=401]  loss = 0.2461\n",
      "[iter=501]  loss = 0.2470\n",
      "[iter=601]  loss = 0.2466\n",
      "[iter=701]  loss = 0.2467\n",
      "[iter=801]  loss = 0.2466\n",
      "[iter=901]  loss = 0.2463\n",
      "[iter=1001]  loss = 0.2462\n",
      "[iter=1101]  loss = 0.2465\n",
      "[iter=1201]  loss = 0.2462\n",
      "[iter=1301]  loss = 0.2461\n",
      "[iter=1401]  loss = 0.2463\n",
      "[iter=1501]  loss = 0.2460\n",
      "[iter=1601]  loss = 0.2459\n",
      "[iter=1701]  loss = 0.2462\n",
      "[iter=1801]  loss = 0.2456\n",
      "[iter=1901]  loss = 0.2460\n",
      "[iter=2001]  loss = 0.2462\n",
      "[iter=2101]  loss = 0.2461\n",
      "[iter=2201]  loss = 0.2454\n",
      "[iter=2301]  loss = 0.2453\n",
      "[iter=2401]  loss = 0.2459\n",
      "[iter=2501]  loss = 0.2457\n",
      "[iter=2601]  loss = 0.2457\n",
      "[iter=2701]  loss = 0.2449\n",
      "[iter=2801]  loss = 0.2462\n",
      "[iter=2901]  loss = 0.2448\n",
      "[iter=3001]  loss = 0.2457\n",
      "[iter=3101]  loss = 0.2457\n",
      "[iter=3201]  loss = 0.2452\n",
      "[iter=3301]  loss = 0.2454\n",
      "[iter=3401]  loss = 0.2451\n",
      "[iter=3501]  loss = 0.2451\n",
      "[iter=3601]  loss = 0.2452\n",
      "[iter=3701]  loss = 0.2447\n",
      "[iter=3801]  loss = 0.2455\n",
      "[iter=3901]  loss = 0.2455\n",
      "[iter=4001]  loss = 0.2447\n",
      "[iter=4101]  loss = 0.2436\n",
      "[iter=4201]  loss = 0.2448\n",
      "[iter=4301]  loss = 0.2450\n",
      "[iter=4401]  loss = 0.2446\n",
      "[iter=4501]  loss = 0.2447\n",
      "[iter=4601]  loss = 0.2445\n",
      "[iter=4701]  loss = 0.2442\n",
      "[iter=4801]  loss = 0.2450\n",
      "[iter=4901]  loss = 0.2441\n",
      "[iter=5001]  loss = 0.2442\n",
      "[iter=5101]  loss = 0.2434\n",
      "[iter=5201]  loss = 0.2450\n",
      "[iter=5301]  loss = 0.2444\n",
      "[iter=5401]  loss = 0.2444\n",
      "[iter=5501]  loss = 0.2440\n",
      "[iter=5601]  loss = 0.2442\n",
      "[iter=5701]  loss = 0.2437\n",
      "[iter=5801]  loss = 0.2439\n",
      "[iter=5901]  loss = 0.2436\n",
      "[iter=6001]  loss = 0.2436\n",
      "[iter=6101]  loss = 0.2434\n",
      "[iter=6201]  loss = 0.2445\n",
      "[iter=6301]  loss = 0.2435\n",
      "[iter=6401]  loss = 0.2433\n",
      "[iter=6501]  loss = 0.2435\n",
      "[iter=6601]  loss = 0.2433\n",
      "[iter=6701]  loss = 0.2431\n",
      "[iter=6801]  loss = 0.2430\n",
      "[iter=6901]  loss = 0.2429\n",
      "[iter=7001]  loss = 0.2436\n",
      "[iter=7101]  loss = 0.2435\n",
      "[iter=7201]  loss = 0.2434\n",
      "[iter=7301]  loss = 0.2418\n",
      "[iter=7401]  loss = 0.2420\n",
      "[iter=7501]  loss = 0.2424\n",
      "[iter=7601]  loss = 0.2417\n",
      "[iter=7701]  loss = 0.2420\n",
      "[iter=7801]  loss = 0.2423\n",
      "[iter=7901]  loss = 0.2420\n",
      "[iter=8001]  loss = 0.2429\n",
      "[iter=8101]  loss = 0.2419\n",
      "[iter=8201]  loss = 0.2422\n",
      "[iter=8301]  loss = 0.2413\n",
      "[iter=8401]  loss = 0.2426\n",
      "[iter=8501]  loss = 0.2424\n",
      "[iter=8601]  loss = 0.2419\n",
      "[iter=8701]  loss = 0.2422\n",
      "[iter=8801]  loss = 0.2412\n",
      "[iter=8901]  loss = 0.2409\n",
      "[iter=9001]  loss = 0.2417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=9101]  loss = 0.2410\n",
      "[iter=9201]  loss = 0.2413\n",
      "[iter=9301]  loss = 0.2414\n",
      "[iter=9401]  loss = 0.2422\n",
      "[iter=9501]  loss = 0.2411\n",
      "[iter=9601]  loss = 0.2409\n",
      "[iter=9701]  loss = 0.2409\n",
      "[iter=9801]  loss = 0.2411\n",
      "[iter=9901]  loss = 0.2403\n",
      "[iter=10001]  loss = 0.2409\n",
      "[iter=10101]  loss = 0.2408\n",
      "[iter=10201]  loss = 0.2407\n",
      "[iter=10301]  loss = 0.2415\n",
      "[iter=10401]  loss = 0.2394\n",
      "[iter=10501]  loss = 0.2385\n",
      "[iter=10601]  loss = 0.2399\n",
      "[iter=10701]  loss = 0.2409\n",
      "[iter=10801]  loss = 0.2407\n",
      "[iter=10901]  loss = 0.2394\n",
      "[iter=11001]  loss = 0.2404\n",
      "[iter=11101]  loss = 0.2400\n",
      "[iter=11201]  loss = 0.2397\n",
      "[iter=11301]  loss = 0.2387\n",
      "[iter=11401]  loss = 0.2388\n",
      "[iter=11501]  loss = 0.2387\n",
      "[iter=11601]  loss = 0.2389\n",
      "[iter=11701]  loss = 0.2386\n",
      "[iter=11801]  loss = 0.2385\n",
      "[iter=11901]  loss = 0.2380\n",
      "[iter=12001]  loss = 0.2385\n",
      "[iter=12101]  loss = 0.2388\n",
      "[iter=12201]  loss = 0.2392\n",
      "[iter=12301]  loss = 0.2384\n",
      "[iter=12401]  loss = 0.2381\n",
      "[iter=12501]  loss = 0.2384\n",
      "[iter=12601]  loss = 0.2378\n",
      "[iter=12701]  loss = 0.2375\n",
      "[iter=12801]  loss = 0.2372\n",
      "[iter=12901]  loss = 0.2370\n",
      "[iter=13001]  loss = 0.2376\n",
      "[iter=13101]  loss = 0.2373\n",
      "[iter=13201]  loss = 0.2374\n",
      "[iter=13301]  loss = 0.2363\n",
      "[iter=13401]  loss = 0.2356\n",
      "[iter=13501]  loss = 0.2355\n",
      "[iter=13601]  loss = 0.2361\n",
      "[iter=13701]  loss = 0.2363\n",
      "[iter=13801]  loss = 0.2362\n",
      "[iter=13901]  loss = 0.2358\n",
      "[iter=14001]  loss = 0.2337\n",
      "[iter=14101]  loss = 0.2356\n",
      "[iter=14201]  loss = 0.2351\n",
      "[iter=14301]  loss = 0.2342\n",
      "[iter=14401]  loss = 0.2353\n",
      "[iter=14501]  loss = 0.2349\n",
      "[iter=14601]  loss = 0.2343\n",
      "[iter=14701]  loss = 0.2351\n",
      "[iter=14801]  loss = 0.2340\n",
      "[iter=14901]  loss = 0.2336\n",
      "[iter=15001]  loss = 0.2331\n",
      "[iter=15101]  loss = 0.2338\n",
      "[iter=15201]  loss = 0.2335\n",
      "[iter=15301]  loss = 0.2324\n",
      "[iter=15401]  loss = 0.2321\n",
      "[iter=15501]  loss = 0.2324\n",
      "[iter=15601]  loss = 0.2337\n",
      "[iter=15701]  loss = 0.2330\n",
      "[iter=15801]  loss = 0.2319\n",
      "[iter=15901]  loss = 0.2310\n",
      "[iter=16001]  loss = 0.2307\n",
      "[iter=16101]  loss = 0.2322\n",
      "[iter=16201]  loss = 0.2319\n",
      "[iter=16301]  loss = 0.2306\n",
      "[iter=16401]  loss = 0.2311\n",
      "[iter=16501]  loss = 0.2299\n",
      "[iter=16601]  loss = 0.2324\n",
      "[iter=16701]  loss = 0.2292\n",
      "[iter=16801]  loss = 0.2311\n",
      "[iter=16901]  loss = 0.2307\n",
      "[iter=17001]  loss = 0.2304\n",
      "[iter=17101]  loss = 0.2300\n",
      "[iter=17201]  loss = 0.2286\n",
      "[iter=17301]  loss = 0.2289\n",
      "[iter=17401]  loss = 0.2280\n",
      "[iter=17501]  loss = 0.2291\n",
      "[iter=17601]  loss = 0.2268\n",
      "[iter=17701]  loss = 0.2282\n",
      "[iter=17801]  loss = 0.2273\n",
      "[iter=17901]  loss = 0.2282\n",
      "[iter=18001]  loss = 0.2299\n",
      "[iter=18101]  loss = 0.2250\n",
      "[iter=18201]  loss = 0.2264\n",
      "[iter=18301]  loss = 0.2276\n",
      "[iter=18401]  loss = 0.2256\n",
      "[iter=18501]  loss = 0.2278\n",
      "[iter=18601]  loss = 0.2265\n",
      "[iter=18701]  loss = 0.2258\n",
      "[iter=18801]  loss = 0.2255\n",
      "[iter=18901]  loss = 0.2264\n",
      "[iter=19001]  loss = 0.2259\n",
      "[iter=19101]  loss = 0.2253\n",
      "[iter=19201]  loss = 0.2236\n",
      "[iter=19301]  loss = 0.2270\n",
      "[iter=19401]  loss = 0.2264\n",
      "[iter=19501]  loss = 0.2225\n",
      "[iter=19601]  loss = 0.2262\n",
      "[iter=19701]  loss = 0.2259\n",
      "[iter=19801]  loss = 0.2254\n",
      "[iter=19901]  loss = 0.2236\n",
      "[iter=20001]  loss = 0.2228\n",
      "[iter=20101]  loss = 0.2221\n",
      "[iter=20201]  loss = 0.2220\n",
      "[iter=20301]  loss = 0.2237\n",
      "[iter=20401]  loss = 0.2212\n",
      "[iter=20501]  loss = 0.2231\n",
      "[iter=20601]  loss = 0.2229\n",
      "[iter=20701]  loss = 0.2225\n",
      "[iter=20801]  loss = 0.2216\n",
      "[iter=20901]  loss = 0.2225\n",
      "[iter=21001]  loss = 0.2222\n",
      "[iter=21101]  loss = 0.2209\n",
      "[iter=21201]  loss = 0.2207\n",
      "[iter=21301]  loss = 0.2211\n",
      "[iter=21401]  loss = 0.2183\n",
      "[iter=21501]  loss = 0.2212\n",
      "[iter=21601]  loss = 0.2198\n",
      "[iter=21701]  loss = 0.2200\n",
      "[iter=21801]  loss = 0.2196\n",
      "[iter=21901]  loss = 0.2190\n",
      "[iter=22001]  loss = 0.2184\n",
      "[iter=22101]  loss = 0.2155\n",
      "[iter=22201]  loss = 0.2171\n",
      "[iter=22301]  loss = 0.2166\n",
      "[iter=22401]  loss = 0.2167\n",
      "[iter=22501]  loss = 0.2171\n",
      "[iter=22601]  loss = 0.2161\n",
      "[iter=22701]  loss = 0.2167\n",
      "[iter=22801]  loss = 0.2156\n",
      "[iter=22901]  loss = 0.2152\n",
      "[iter=23001]  loss = 0.2131\n",
      "[iter=23101]  loss = 0.2128\n",
      "[iter=23201]  loss = 0.2155\n",
      "[iter=23301]  loss = 0.2149\n",
      "[iter=23401]  loss = 0.2127\n",
      "[iter=23501]  loss = 0.2140\n",
      "[iter=23601]  loss = 0.2131\n",
      "[iter=23701]  loss = 0.2142\n",
      "[iter=23801]  loss = 0.2144\n",
      "[iter=23901]  loss = 0.2108\n",
      "[iter=24001]  loss = 0.2158\n",
      "[iter=24101]  loss = 0.2135\n",
      "[iter=24201]  loss = 0.2133\n",
      "[iter=24301]  loss = 0.2117\n",
      "[iter=24401]  loss = 0.2133\n",
      "[iter=24501]  loss = 0.2115\n",
      "[iter=24601]  loss = 0.2125\n",
      "[iter=24701]  loss = 0.2123\n",
      "[iter=24801]  loss = 0.2079\n",
      "[iter=24901]  loss = 0.2134\n",
      "[iter=25001]  loss = 0.2116\n",
      "[iter=25101]  loss = 0.2086\n",
      "[iter=25201]  loss = 0.2085\n",
      "[iter=25301]  loss = 0.2069\n",
      "[iter=25401]  loss = 0.2037\n",
      "[iter=25501]  loss = 0.2105\n",
      "[iter=25601]  loss = 0.2053\n",
      "[iter=25701]  loss = 0.2085\n",
      "[iter=25801]  loss = 0.2094\n",
      "[iter=25901]  loss = 0.2072\n",
      "[iter=26001]  loss = 0.2096\n",
      "[iter=26101]  loss = 0.2056\n",
      "[iter=26201]  loss = 0.2079\n",
      "[iter=26301]  loss = 0.2052\n",
      "[iter=26401]  loss = 0.2042\n",
      "[iter=26501]  loss = 0.2061\n",
      "[iter=26601]  loss = 0.2050\n",
      "[iter=26701]  loss = 0.2064\n",
      "[iter=26801]  loss = 0.2041\n",
      "[iter=26901]  loss = 0.2026\n",
      "[iter=27001]  loss = 0.2037\n",
      "[iter=27101]  loss = 0.2023\n",
      "[iter=27201]  loss = 0.2040\n",
      "[iter=27301]  loss = 0.2039\n",
      "[iter=27401]  loss = 0.2038\n",
      "[iter=27501]  loss = 0.2030\n",
      "[iter=27601]  loss = 0.2025\n",
      "[iter=27701]  loss = 0.2000\n",
      "[iter=27801]  loss = 0.2016\n",
      "[iter=27901]  loss = 0.2047\n",
      "[iter=28001]  loss = 0.2014\n",
      "[iter=28101]  loss = 0.2010\n",
      "[iter=28201]  loss = 0.2038\n",
      "[iter=28301]  loss = 0.2006\n",
      "[iter=28401]  loss = 0.1988\n",
      "[iter=28501]  loss = 0.2020\n",
      "[iter=28601]  loss = 0.1996\n",
      "[iter=28701]  loss = 0.1963\n",
      "[iter=28801]  loss = 0.1980\n",
      "[iter=28901]  loss = 0.1970\n",
      "[iter=29001]  loss = 0.1989\n",
      "[iter=29101]  loss = 0.1964\n",
      "[iter=29201]  loss = 0.1969\n",
      "[iter=29301]  loss = 0.1962\n",
      "[iter=29401]  loss = 0.1982\n",
      "[iter=29501]  loss = 0.1981\n",
      "[iter=29601]  loss = 0.1941\n",
      "[iter=29701]  loss = 0.1961\n",
      "[iter=29801]  loss = 0.1935\n",
      "[iter=29901]  loss = 0.1944\n",
      "[iter=30001]  loss = 0.1979\n",
      "[iter=30101]  loss = 0.1934\n",
      "[iter=30201]  loss = 0.1911\n",
      "[iter=30301]  loss = 0.1967\n",
      "[iter=30401]  loss = 0.1975\n",
      "[iter=30501]  loss = 0.1920\n",
      "[iter=30601]  loss = 0.1938\n",
      "[iter=30701]  loss = 0.1952\n",
      "[iter=30801]  loss = 0.1901\n",
      "[iter=30901]  loss = 0.1961\n",
      "[iter=31001]  loss = 0.1950\n",
      "[iter=31101]  loss = 0.1896\n",
      "[iter=31201]  loss = 0.1912\n",
      "[iter=31301]  loss = 0.1938\n",
      "[iter=31401]  loss = 0.1918\n",
      "[iter=31501]  loss = 0.1938\n",
      "[iter=31601]  loss = 0.1946\n",
      "[iter=31701]  loss = 0.1936\n",
      "[iter=31801]  loss = 0.1930\n",
      "[iter=31901]  loss = 0.1894\n",
      "[iter=32001]  loss = 0.1901\n",
      "[iter=32101]  loss = 0.1934\n",
      "[iter=32201]  loss = 0.1885\n",
      "[iter=32301]  loss = 0.1903\n",
      "[iter=32401]  loss = 0.1906\n",
      "[iter=32501]  loss = 0.1873\n",
      "[iter=32601]  loss = 0.1894\n",
      "[iter=32701]  loss = 0.1881\n",
      "[iter=32801]  loss = 0.1868\n",
      "[iter=32901]  loss = 0.1880\n",
      "[iter=33001]  loss = 0.1857\n",
      "[iter=33101]  loss = 0.1863\n",
      "[iter=33201]  loss = 0.1900\n",
      "[iter=33301]  loss = 0.1876\n",
      "[iter=33401]  loss = 0.1834\n",
      "[iter=33501]  loss = 0.1832\n",
      "[iter=33601]  loss = 0.1863\n",
      "[iter=33701]  loss = 0.1849\n",
      "[iter=33801]  loss = 0.1888\n",
      "[iter=33901]  loss = 0.1893\n",
      "[iter=34001]  loss = 0.1858\n",
      "[iter=34101]  loss = 0.1801\n",
      "[iter=34201]  loss = 0.1823\n",
      "[iter=34301]  loss = 0.1827\n",
      "[iter=34401]  loss = 0.1801\n",
      "[iter=34501]  loss = 0.1818\n",
      "[iter=34601]  loss = 0.1797\n",
      "[iter=34701]  loss = 0.1839\n",
      "[iter=34801]  loss = 0.1860\n",
      "[iter=34901]  loss = 0.1799\n",
      "[iter=35001]  loss = 0.1866\n",
      "[iter=35101]  loss = 0.1766\n",
      "[iter=35201]  loss = 0.1831\n",
      "[iter=35301]  loss = 0.1810\n",
      "[iter=35401]  loss = 0.1809\n",
      "[iter=35501]  loss = 0.1793\n",
      "[iter=35601]  loss = 0.1836\n",
      "[iter=35701]  loss = 0.1813\n",
      "[iter=35801]  loss = 0.1791\n",
      "[iter=35901]  loss = 0.1756\n",
      "[iter=36001]  loss = 0.1821\n",
      "[iter=36101]  loss = 0.1821\n",
      "[iter=36201]  loss = 0.1789\n",
      "[iter=36301]  loss = 0.1791\n",
      "[iter=36401]  loss = 0.1777\n",
      "[iter=36501]  loss = 0.1746\n",
      "[iter=36601]  loss = 0.1747\n",
      "[iter=36701]  loss = 0.1747\n",
      "[iter=36801]  loss = 0.1744\n",
      "[iter=36901]  loss = 0.1788\n",
      "[iter=37001]  loss = 0.1753\n",
      "[iter=37101]  loss = 0.1748\n",
      "[iter=37201]  loss = 0.1783\n",
      "[iter=37301]  loss = 0.1735\n",
      "[iter=37401]  loss = 0.1749\n",
      "[iter=37501]  loss = 0.1731\n",
      "[iter=37601]  loss = 0.1759\n",
      "[iter=37701]  loss = 0.1727\n",
      "[iter=37801]  loss = 0.1765\n",
      "[iter=37901]  loss = 0.1744\n",
      "[iter=38001]  loss = 0.1713\n",
      "[iter=38101]  loss = 0.1714\n",
      "[iter=38201]  loss = 0.1727\n",
      "[iter=38301]  loss = 0.1784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=38401]  loss = 0.1754\n",
      "[iter=38501]  loss = 0.1712\n",
      "[iter=38601]  loss = 0.1722\n",
      "[iter=38701]  loss = 0.1807\n",
      "[iter=38801]  loss = 0.1724\n",
      "[iter=38901]  loss = 0.1679\n",
      "[iter=39001]  loss = 0.1746\n",
      "[iter=39101]  loss = 0.1732\n",
      "[iter=39201]  loss = 0.1686\n",
      "[iter=39301]  loss = 0.1677\n",
      "[iter=39401]  loss = 0.1759\n",
      "[iter=39501]  loss = 0.1737\n",
      "[iter=39601]  loss = 0.1687\n",
      "[iter=39701]  loss = 0.1760\n",
      "[iter=39801]  loss = 0.1719\n",
      "[iter=39901]  loss = 0.1683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ef505e1880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss3 = cgp2.run_training_loop_multi_neuron_model(training_data2,0.9,0.99)\n",
    "plt.plot(loss3,label = \"Adam loss\")\n",
    "\n",
    "\n",
    "loss1 = cgp1.run_training_loop_multi_neuron_model(training_data1)\n",
    "plt.plot(loss1,label = \"SGD loss\")\n",
    "\n",
    "loss2 = cgp1.run_training_loop_multi_neuron_model(training_data1,0.9,True)\n",
    "plt.plot(loss2,label = \"SGD+ loss\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iterations in hundreds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Multi neuron loss using different optimizers\")\n",
    "\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28622b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
