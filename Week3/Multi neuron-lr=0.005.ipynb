{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7ddb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ComputationalGraphPrimer import *\n",
    "import operator\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541f9ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDPlus(ComputationalGraphPrimer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def run_training_loop_multi_neuron_model(self, training_data,mu=0.0,SGDplus=False):\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how \n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "           \n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of \n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "        \n",
    "        ##My input start\n",
    "        self.bias_update = [0.0]*(self.num_layers+1)\n",
    "        self.step = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1)\n",
    "        self.mu = mu if SGDplus else 0.0      \n",
    "        \n",
    "        ##My input end\n",
    "        \n",
    "        \n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        for i in range(self.training_iterations):\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            self.backprop_and_update_params_multi_neuron_model(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "        return loss_running_record\n",
    "           \n",
    "\n",
    "\n",
    "    def forward_prop_multi_neuron_model(self, data_tuples_in_batch):\n",
    "        \"\"\"\n",
    "        During forward propagation, we push each batch of the input data through the\n",
    "        network.  In order to explain the logic of forward, consider the following network\n",
    "        layout in 4 nodes in the input layer, 2 nodes in the hidden layer, and 1 node in\n",
    "        the output layer.\n",
    "\n",
    "                               input\n",
    "                                  \n",
    "                                 x                                             x = node\n",
    "                                                                            \n",
    "                                 x         x|                                  | = sigmoid activation\n",
    "                                                     x|\n",
    "                                 x         x|   \n",
    "\n",
    "                                 x\n",
    "                            \n",
    "                             layer_0    layer_1    layer_2\n",
    "\n",
    "                \n",
    "        In the code shown below, the expressions to evaluate for computing the\n",
    "        pre-activation values at a node are stored at the layer in which the nodes reside.\n",
    "        That is, the dictionary look-up \"self.layer_exp_objects[layer_index]\" returns the\n",
    "        Expression objects for which the left-side dependent variable is in the layer\n",
    "        pointed to layer_index.  So the example shown above, \"self.layer_exp_objects[1]\"\n",
    "        will return two Expression objects, one for each of the two nodes in the second\n",
    "        layer of the network (that is, layer indexed 1).\n",
    "\n",
    "        The pre-activation values obtained by evaluating the expressions at each node are\n",
    "        then subject to Sigmoid activation, followed by the calculation of the partial\n",
    "        derivative of the output of the Sigmoid function with respect to its input.\n",
    "\n",
    "        In the forward, the values calculated for the nodes in each layer are stored in\n",
    "        the dictionary\n",
    "\n",
    "                        self.forw_prop_vals_at_layers[ layer_index ]\n",
    "\n",
    "        and the gradients values calculated at the same nodes in the dictionary:\n",
    "\n",
    "                        self.gradient_vals_for_layers[ layer_index ]\n",
    "\n",
    "        \"\"\"\n",
    "        self.forw_prop_vals_at_layers = {i : [] for i in range(self.num_layers)}   \n",
    "        self.gradient_vals_for_layers = {i : [] for i in range(1, self.num_layers)}\n",
    "        for vals_for_input_vars in data_tuples_in_batch:\n",
    "            self.forw_prop_vals_at_layers[0].append(vals_for_input_vars)\n",
    "            for layer_index in range(1, self.num_layers):\n",
    "                input_vars = self.layer_vars[layer_index-1]\n",
    "                if layer_index == 1:\n",
    "                    vals_for_input_vars_dict =  dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "                output_vals_arr = []\n",
    "                gradients_val_arr = []\n",
    "                ##  In the following loop for forward propagation calculations, exp_obj is the Exp object\n",
    "                ##  that is created for each user-supplied expression that specifies the network.  See the\n",
    "                ##  definition of the class Exp (for 'Expression') by searching for \"class Exp\":\n",
    "                for exp_obj in self.layer_exp_objects[layer_index]:\n",
    "                    output_val = self.eval_expression(exp_obj.body , vals_for_input_vars_dict,    \n",
    "                                                                 self.vals_for_learnable_params, input_vars)\n",
    "                    ## [Search for \"self.bias\" in this file.]  As mentioned earlier, adding bias to each \n",
    "                    ##  layer improves class discrimination:\n",
    "                    output_val = output_val + self.bias[layer_index-1]                \n",
    "                    ## apply sigmoid activation:\n",
    "                    output_val = 1.0 / (1.0 + np.exp(-1.0 * output_val))\n",
    "                    output_vals_arr.append(output_val)\n",
    "                    ## calculate partial of the activation function as a function of its input\n",
    "                    deriv_sigmoid = output_val * (1.0 - output_val)\n",
    "                    gradients_val_arr.append(deriv_sigmoid)\n",
    "                    vals_for_input_vars_dict[ exp_obj.dependent_var ] = output_val\n",
    "                self.forw_prop_vals_at_layers[layer_index].append(output_vals_arr)\n",
    "                ##  See the bullets in red on Slides 70 and 73 of my Week 3 slides for what needs\n",
    "                ##  to be stored during the forward propagation of data through the network:\n",
    "                self.gradient_vals_for_layers[layer_index].append(gradients_val_arr)\n",
    "\n",
    "\n",
    "    def backprop_and_update_params_multi_neuron_model(self, y_error, class_labels):\n",
    "        \"\"\"\n",
    "        First note that loop index variable 'back_layer_index' starts with the index of\n",
    "        the last layer.  For the 3-layer example shown for 'forward', back_layer_index\n",
    "        starts with a value of 2, its next value is 1, and that's it.\n",
    "\n",
    "        Stochastic Gradient Gradient calls for the backpropagated loss to be averaged over\n",
    "        the samples in a batch.  To explain how this averaging is carried out by the\n",
    "        backprop function, consider the last node on the example shown in the forward()\n",
    "        function above.  Standing at the node, we look at the 'input' values stored in the\n",
    "        variable \"input_vals\".  Assuming a batch size of 8, this will be list of\n",
    "        lists. Each of the inner lists will have two values for the two nodes in the\n",
    "        hidden layer. And there will be 8 of these for the 8 elements of the batch.  We average\n",
    "        these values 'input vals' and store those in the variable \"input_vals_avg\".  Next we\n",
    "        must carry out the same batch-based averaging for the partial derivatives stored in the\n",
    "        variable \"deriv_sigmoid\".\n",
    "\n",
    "        Pay attention to the variable 'vars_in_layer'.  These store the node variables in\n",
    "        the current layer during backpropagation.  Since back_layer_index starts with a\n",
    "        value of 2, the variable 'vars_in_layer' will have just the single node for the\n",
    "        example shown for forward(). With respect to what is stored in vars_in_layer', the\n",
    "        variables stored in 'input_vars_to_layer' correspond to the input layer with\n",
    "        respect to the current layer. \n",
    "        \"\"\"\n",
    "        # backproped prediction error:\n",
    "        pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "        pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "        for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "            input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "            input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "            input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "            deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "            deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "            deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                             [float(len(class_labels))] * len(class_labels)))\n",
    "            vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "            vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "            layer_params = self.layer_params[back_layer_index]         \n",
    "            ## note that layer_params are stored in a dict like        \n",
    "                ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "            ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "            transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "            backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "            for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                for j,var2 in enumerate(vars_in_layer):\n",
    "                    backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                               pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                               for i in range(len(vars_in_layer))])\n",
    "#                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "            pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "            input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "            for j,var in enumerate(vars_in_layer):\n",
    "                layer_params = self.layer_params[back_layer_index][j]\n",
    "                ##  Regarding the parameter update loop that follows, see the Slides 74 through 77 of my Week 3 \n",
    "                ##  lecture slides for how the parameters are updated using the partial derivatives stored away \n",
    "                ##  during forward propagation of data. The theory underlying these calculations is presented \n",
    "                ##  in Slides 68 through 71. \n",
    "                for i,param in enumerate(layer_params):\n",
    "                    gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j]\n",
    "                    \n",
    "                    #My change start\n",
    "                    self.step[back_layer_index-1][i] = (self.mu*self.step[back_layer_index-1][i]) +  gradient_of_loss_for_param * deriv_sigmoid_avg[j]\n",
    "                       \n",
    "                    \n",
    "                    self.vals_for_learnable_params[param] += self.step[back_layer_index-1][i]*self.learning_rate\n",
    "                    \n",
    "                    \n",
    "            self.bias_update[back_layer_index-1] = (self.mu*self.bias_update[back_layer_index-1]) + sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                           * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg)       \n",
    "            self.bias[back_layer_index-1] += self.learning_rate * self.bias_update[back_layer_index-1]\n",
    "            \n",
    "            ##My change end\n",
    "    ######################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbabdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(ComputationalGraphPrimer):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def run_training_loop_multi_neuron_model(self, training_data,beta1,beta2):\n",
    "\n",
    "        class DataLoader:\n",
    "            \"\"\"\n",
    "            To understand the logic of the dataloader, it would help if you first understand how \n",
    "            the training dataset is created.  Search for the following function in this file:\n",
    "\n",
    "                             gen_training_data(self)\n",
    "           \n",
    "            As you will see in the implementation code for this method, the training dataset\n",
    "            consists of a Python dict with two keys, 0 and 1, the former points to a list of \n",
    "            all Class 0 samples and the latter to a list of all Class 1 samples.  In each list,\n",
    "            the data samples are drawn from a multi-dimensional Gaussian distribution.  The two\n",
    "            classes have different means and variances.  The dimensionality of each data sample\n",
    "            is set by the number of nodes in the input layer of the neural network.\n",
    "\n",
    "            The data loader's job is to construct a batch of samples drawn randomly from the two\n",
    "            lists mentioned above.  And it mush also associate the class label with each sample\n",
    "            separately.\n",
    "            \"\"\"\n",
    "            def __init__(self, training_data, batch_size):\n",
    "                self.training_data = training_data\n",
    "                self.batch_size = batch_size\n",
    "                self.class_0_samples = [(item, 0) for item in self.training_data[0]]    ## Associate label 0 with each sample\n",
    "                self.class_1_samples = [(item, 1) for item in self.training_data[1]]    ## Associate label 1 with each sample\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.training_data[0]) + len(self.training_data[1])\n",
    "\n",
    "            def _getitem(self):    \n",
    "                cointoss = random.choice([0,1])                            ## When a batch is created by getbatch(), we want the\n",
    "                                                                           ##   samples to be chosen randomly from the two lists\n",
    "                if cointoss == 0:\n",
    "                    return random.choice(self.class_0_samples)\n",
    "                else:\n",
    "                    return random.choice(self.class_1_samples)            \n",
    "\n",
    "            def getbatch(self):\n",
    "                batch_data,batch_labels = [],[]                            ## First list for samples, the second for labels\n",
    "                maxval = 0.0                                               ## For approximate batch data normalization\n",
    "                for _ in range(self.batch_size):\n",
    "                    item = self._getitem()\n",
    "                    if np.max(item[0]) > maxval: \n",
    "                        maxval = np.max(item[0])\n",
    "                    batch_data.append(item[0])\n",
    "                    batch_labels.append(item[1])\n",
    "                batch_data = [item/maxval for item in batch_data]          ## Normalize batch data       \n",
    "                batch = [batch_data, batch_labels]\n",
    "                return batch                \n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        The training loop must first initialize the learnable parameters.  Remember, these are the \n",
    "        symbolic names in your input expressions for the neural layer that do not begin with the \n",
    "        letter 'x'.  In this case, we are initializing with random numbers from a uniform distribution \n",
    "        over the interval (0,1).\n",
    "        \"\"\"\n",
    "        self.vals_for_learnable_params = {param: random.uniform(0,1) for param in self.learnable_params}\n",
    "\n",
    "        self.bias = [random.uniform(0,1) for _ in range(self.num_layers-1)]      ## Adding the bias to each layer improves \n",
    "                                                                                 ##   class discrimination. We initialize it \n",
    "                                                                                 ##   to a random number.\n",
    "        \n",
    "        ##My input start\n",
    "        self.bias_m = [0.0]*(self.num_layers+1)\n",
    "        self.bias_v = [0.0]*(self.num_layers+1)\n",
    "        self.bias_mh = [0.0]*(self.num_layers+1)\n",
    "        self.bias_vh = [0.0]*(self.num_layers+1)\n",
    "        \n",
    "        self.step_m = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1)\n",
    "        self.step_v = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1) \n",
    "        self.step_mh = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1)\n",
    "        self.step_vh = [[0]*(len(self.learnable_params)+1)]*(self.num_layers+1) \n",
    "        \n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2  \n",
    "        self.m = 0\n",
    "        \n",
    "        ##My input end\n",
    "        \n",
    "        \n",
    "        data_loader = DataLoader(training_data, batch_size=self.batch_size)\n",
    "        loss_running_record = []\n",
    "        i = 0\n",
    "        avg_loss_over_iterations = 0.0                                          ##  Average the loss over iterations for printing out \n",
    "                                                                                 ##    every N iterations during the training loop.   \n",
    "        for i in range(self.training_iterations):\n",
    "            self.m = i+1\n",
    "            data = data_loader.getbatch()\n",
    "            data_tuples = data[0]\n",
    "            class_labels = data[1]\n",
    "            self.forward_prop_multi_neuron_model(data_tuples)                                  ## FORW PROP works by side-effect \n",
    "            predicted_labels_for_batch = self.forw_prop_vals_at_layers[self.num_layers-1]      ## Predictions from FORW PROP\n",
    "            y_preds =  [item for sublist in  predicted_labels_for_batch  for item in sublist]  ## Get numeric vals for predictions\n",
    "            loss = sum([(abs(class_labels[i] - y_preds[i]))**2 for i in range(len(class_labels))])  ## Calculate loss for batch\n",
    "            loss_avg = loss / float(len(class_labels))                                         ## Average the loss over batch\n",
    "            avg_loss_over_iterations += loss_avg                                              ## Add to Average loss over iterations\n",
    "            if i%(self.display_loss_how_often) == 0: \n",
    "                avg_loss_over_iterations /= self.display_loss_how_often\n",
    "                loss_running_record.append(avg_loss_over_iterations)\n",
    "                print(\"[iter=%d]  loss = %.4f\" %  (i+1, avg_loss_over_iterations))            ## Display avg loss\n",
    "                avg_loss_over_iterations = 0.0                                                ## Re-initialize avg-over-iterations loss\n",
    "            y_errors = list(map(operator.sub, class_labels, y_preds))\n",
    "            y_error_avg = sum(y_errors) / float(len(class_labels))\n",
    "            self.backprop_and_update_params_multi_neuron_model(y_error_avg, class_labels)      ## BACKPROP loss\n",
    "        return loss_running_record\n",
    "           \n",
    "\n",
    "\n",
    "    def forward_prop_multi_neuron_model(self, data_tuples_in_batch):\n",
    "        \"\"\"\n",
    "        During forward propagation, we push each batch of the input data through the\n",
    "        network.  In order to explain the logic of forward, consider the following network\n",
    "        layout in 4 nodes in the input layer, 2 nodes in the hidden layer, and 1 node in\n",
    "        the output layer.\n",
    "\n",
    "                               input\n",
    "                                  \n",
    "                                 x                                             x = node\n",
    "                                                                            \n",
    "                                 x         x|                                  | = sigmoid activation\n",
    "                                                     x|\n",
    "                                 x         x|   \n",
    "\n",
    "                                 x\n",
    "                            \n",
    "                             layer_0    layer_1    layer_2\n",
    "\n",
    "                \n",
    "        In the code shown below, the expressions to evaluate for computing the\n",
    "        pre-activation values at a node are stored at the layer in which the nodes reside.\n",
    "        That is, the dictionary look-up \"self.layer_exp_objects[layer_index]\" returns the\n",
    "        Expression objects for which the left-side dependent variable is in the layer\n",
    "        pointed to layer_index.  So the example shown above, \"self.layer_exp_objects[1]\"\n",
    "        will return two Expression objects, one for each of the two nodes in the second\n",
    "        layer of the network (that is, layer indexed 1).\n",
    "\n",
    "        The pre-activation values obtained by evaluating the expressions at each node are\n",
    "        then subject to Sigmoid activation, followed by the calculation of the partial\n",
    "        derivative of the output of the Sigmoid function with respect to its input.\n",
    "\n",
    "        In the forward, the values calculated for the nodes in each layer are stored in\n",
    "        the dictionary\n",
    "\n",
    "                        self.forw_prop_vals_at_layers[ layer_index ]\n",
    "\n",
    "        and the gradients values calculated at the same nodes in the dictionary:\n",
    "\n",
    "                        self.gradient_vals_for_layers[ layer_index ]\n",
    "\n",
    "        \"\"\"\n",
    "        self.forw_prop_vals_at_layers = {i : [] for i in range(self.num_layers)}   \n",
    "        self.gradient_vals_for_layers = {i : [] for i in range(1, self.num_layers)}\n",
    "        for vals_for_input_vars in data_tuples_in_batch:\n",
    "            self.forw_prop_vals_at_layers[0].append(vals_for_input_vars)\n",
    "            for layer_index in range(1, self.num_layers):\n",
    "                input_vars = self.layer_vars[layer_index-1]\n",
    "                if layer_index == 1:\n",
    "                    vals_for_input_vars_dict =  dict(zip(input_vars, list(vals_for_input_vars)))\n",
    "                output_vals_arr = []\n",
    "                gradients_val_arr = []\n",
    "                ##  In the following loop for forward propagation calculations, exp_obj is the Exp object\n",
    "                ##  that is created for each user-supplied expression that specifies the network.  See the\n",
    "                ##  definition of the class Exp (for 'Expression') by searching for \"class Exp\":\n",
    "                for exp_obj in self.layer_exp_objects[layer_index]:\n",
    "                    output_val = self.eval_expression(exp_obj.body , vals_for_input_vars_dict,    \n",
    "                                                                 self.vals_for_learnable_params, input_vars)\n",
    "                    ## [Search for \"self.bias\" in this file.]  As mentioned earlier, adding bias to each \n",
    "                    ##  layer improves class discrimination:\n",
    "                    output_val = output_val + self.bias[layer_index-1]                \n",
    "                    ## apply sigmoid activation:\n",
    "                    output_val = 1.0 / (1.0 + np.exp(-1.0 * output_val))\n",
    "                    output_vals_arr.append(output_val)\n",
    "                    ## calculate partial of the activation function as a function of its input\n",
    "                    deriv_sigmoid = output_val * (1.0 - output_val)\n",
    "                    gradients_val_arr.append(deriv_sigmoid)\n",
    "                    vals_for_input_vars_dict[ exp_obj.dependent_var ] = output_val\n",
    "                self.forw_prop_vals_at_layers[layer_index].append(output_vals_arr)\n",
    "                ##  See the bullets in red on Slides 70 and 73 of my Week 3 slides for what needs\n",
    "                ##  to be stored during the forward propagation of data through the network:\n",
    "                self.gradient_vals_for_layers[layer_index].append(gradients_val_arr)\n",
    "\n",
    "\n",
    "    def backprop_and_update_params_multi_neuron_model(self, y_error, class_labels):\n",
    "        \"\"\"\n",
    "        First note that loop index variable 'back_layer_index' starts with the index of\n",
    "        the last layer.  For the 3-layer example shown for 'forward', back_layer_index\n",
    "        starts with a value of 2, its next value is 1, and that's it.\n",
    "\n",
    "        Stochastic Gradient Gradient calls for the backpropagated loss to be averaged over\n",
    "        the samples in a batch.  To explain how this averaging is carried out by the\n",
    "        backprop function, consider the last node on the example shown in the forward()\n",
    "        function above.  Standing at the node, we look at the 'input' values stored in the\n",
    "        variable \"input_vals\".  Assuming a batch size of 8, this will be list of\n",
    "        lists. Each of the inner lists will have two values for the two nodes in the\n",
    "        hidden layer. And there will be 8 of these for the 8 elements of the batch.  We average\n",
    "        these values 'input vals' and store those in the variable \"input_vals_avg\".  Next we\n",
    "        must carry out the same batch-based averaging for the partial derivatives stored in the\n",
    "        variable \"deriv_sigmoid\".\n",
    "\n",
    "        Pay attention to the variable 'vars_in_layer'.  These store the node variables in\n",
    "        the current layer during backpropagation.  Since back_layer_index starts with a\n",
    "        value of 2, the variable 'vars_in_layer' will have just the single node for the\n",
    "        example shown for forward(). With respect to what is stored in vars_in_layer', the\n",
    "        variables stored in 'input_vars_to_layer' correspond to the input layer with\n",
    "        respect to the current layer. \n",
    "        \"\"\"\n",
    "        # backproped prediction error:\n",
    "        pred_err_backproped_at_layers = {i : [] for i in range(1,self.num_layers-1)}  \n",
    "        pred_err_backproped_at_layers[self.num_layers-1] = [y_error]\n",
    "        for back_layer_index in reversed(range(1,self.num_layers)):\n",
    "            input_vals = self.forw_prop_vals_at_layers[back_layer_index -1]\n",
    "            input_vals_avg = [sum(x) for x in zip(*input_vals)]\n",
    "            input_vals_avg = list(map(operator.truediv, input_vals_avg, [float(len(class_labels))] * len(class_labels)))\n",
    "            deriv_sigmoid =  self.gradient_vals_for_layers[back_layer_index]\n",
    "            deriv_sigmoid_avg = [sum(x) for x in zip(*deriv_sigmoid)]\n",
    "            deriv_sigmoid_avg = list(map(operator.truediv, deriv_sigmoid_avg, \n",
    "                                                             [float(len(class_labels))] * len(class_labels)))\n",
    "            vars_in_layer  =  self.layer_vars[back_layer_index]                 ## a list like ['xo']\n",
    "            vars_in_next_layer_back  =  self.layer_vars[back_layer_index - 1]   ## a list like ['xw', 'xz']\n",
    "\n",
    "            layer_params = self.layer_params[back_layer_index]         \n",
    "            ## note that layer_params are stored in a dict like        \n",
    "                ##     {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
    "            ## \"layer_params[idx]\" is a list of lists for the link weights in layer whose output nodes are in layer \"idx\"\n",
    "            transposed_layer_params = list(zip(*layer_params))         ## creating a transpose of the link matrix\n",
    "\n",
    "            backproped_error = [None] * len(vars_in_next_layer_back)\n",
    "            for k,varr in enumerate(vars_in_next_layer_back):\n",
    "                for j,var2 in enumerate(vars_in_layer):\n",
    "                    backproped_error[k] = sum([self.vals_for_learnable_params[transposed_layer_params[k][i]] * \n",
    "                                               pred_err_backproped_at_layers[back_layer_index][i] \n",
    "                                               for i in range(len(vars_in_layer))])\n",
    "#                                               deriv_sigmoid_avg[i] for i in range(len(vars_in_layer))])\n",
    "            pred_err_backproped_at_layers[back_layer_index - 1]  =  backproped_error\n",
    "            input_vars_to_layer = self.layer_vars[back_layer_index-1]\n",
    "            for j,var in enumerate(vars_in_layer):\n",
    "                layer_params = self.layer_params[back_layer_index][j]\n",
    "                ##  Regarding the parameter update loop that follows, see the Slides 74 through 77 of my Week 3 \n",
    "                ##  lecture slides for how the parameters are updated using the partial derivatives stored away \n",
    "                ##  during forward propagation of data. The theory underlying these calculations is presented \n",
    "                ##  in Slides 68 through 71. \n",
    "                for i,param in enumerate(layer_params):\n",
    "                    gradient_of_loss_for_param = input_vals_avg[i] * pred_err_backproped_at_layers[back_layer_index][j]\n",
    "                    \n",
    "                    #My change start\n",
    "                    self.step_m[back_layer_index-1][i] = (self.beta1*self.step_m[back_layer_index-1][i]) + (1-self.beta1)*(gradient_of_loss_for_param * deriv_sigmoid_avg[j])\n",
    "                    self.step_mh[back_layer_index-1][i] = self.step_m[back_layer_index-1][i]/(1-(self.beta1**self.m))                    \n",
    "                    self.step_v[back_layer_index-1][i] = (self.beta2*self.step_v[back_layer_index-1][i]) + (1-self.beta2)*((gradient_of_loss_for_param * deriv_sigmoid_avg[j])**2)\n",
    "                    self.step_vh[back_layer_index-1][i] = self.step_v[back_layer_index-1][i]/(1-(self.beta2**self.m))                    \n",
    "                    self.vals_for_learnable_params[param] += self.learning_rate * (self.step_mh[back_layer_index-1][i]/(np.sqrt(self.step_vh[back_layer_index-1][i])+10**-6))\n",
    "                    \n",
    "                    \n",
    "            self.bias_m[back_layer_index-1] = (self.beta1*self.bias_m[back_layer_index-1]) + (1-self.beta1)*(sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                           * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg))\n",
    "            self.bias_mh[back_layer_index-1] = self.bias_m[back_layer_index-1]/(1-(self.beta1**self.m))\n",
    "            self.bias_v[back_layer_index-1] = (self.beta2*self.bias_v[back_layer_index-1]) + (1-self.beta2)*((sum(pred_err_backproped_at_layers[back_layer_index]) \\\n",
    "                                                                           * sum(deriv_sigmoid_avg)/len(deriv_sigmoid_avg))**2) \n",
    "            self.bias_vh[back_layer_index-1] = self.bias_v[back_layer_index-1]/(1-(self.beta2**self.m))\n",
    "            self.bias[back_layer_index-1] += self.learning_rate * (self.bias_mh[back_layer_index-1]/(np.sqrt(self.bias_vh[back_layer_index-1])+10**-6)) \n",
    "            \n",
    "            ##My change end\n",
    "    ######################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08452462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xw', 'xz', 'xq', 'xr', 'xs', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'aq', 'as', 'bp', 'br', 'bs', 'bq', 'ar', 'ap'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xr', 'xs', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xw': set(), 'xz': set(), 'xq': {'xw', 'xz'}, 'xr': {'xw', 'xz'}, 'xs': {'xw', 'xz'}, 'xp': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xw', 'xo', 'xz', 'xq', 'xr', 'xs', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'aq', 'cp', 'as', 'bp', 'br', 'bs', 'bq', 'cq', 'ar', 'ap'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xr', 'xs', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xw': {'xo'}, 'xo': set(), 'xz': {'xo'}, 'xq': {'xw', 'xz'}, 'xr': {'xw', 'xz'}, 'xs': {'xw', 'xz'}, 'xp': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xr', 'xs', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001D737713FA0>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001D737713880>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001D7376DA5B0>]}\n"
     ]
    }
   ],
   "source": [
    "cgp1 = SGDPlus(\n",
    "               num_layers = 3,\n",
    "               layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "               expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                              'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                              'xo=cp*xw+cq*xz'],\n",
    "               output_vars = ['xo'],\n",
    "               dataset_size = 5000,\n",
    "               learning_rate = 5e-3,\n",
    "#               learning_rate = 5 * 1e-2,\n",
    "               training_iterations = 40000,\n",
    "               batch_size = 8,\n",
    "               display_loss_how_often = 100,\n",
    "               debug = True,\n",
    "      )\n",
    "\n",
    "cgp1.parse_multi_layer_expressions()\n",
    "training_data1 = cgp1.gen_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd05260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "self.layer_expressions:  {1: ['xw=ap*xp+aq*xq+ar*xr+as*xs', 'xz=bp*xp+bq*xq+br*xr+bs*xs'], 2: ['xo=cp*xw+cq*xz']}\n",
      "\n",
      "\n",
      "[layer index: 1] all variables: {'xw', 'xz', 'xq', 'xr', 'xs', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] learnable params: {'aq', 'as', 'bp', 'br', 'bs', 'bq', 'ar', 'ap'}\n",
      "\n",
      "\n",
      "[layer index: 1] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs']}\n",
      "\n",
      "\n",
      "[layer index: 1] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs'}\n",
      "\n",
      "\n",
      "[layer index: 1] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}}\n",
      "\n",
      "\n",
      "[layer index: 1] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5}\n",
      "\n",
      "\n",
      "[layer index: 1] independent vars: {'xr', 'xs', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 1] leads_to dictionary: {'xw': set(), 'xz': set(), 'xq': {'xw', 'xz'}, 'xr': {'xw', 'xz'}, 'xs': {'xw', 'xz'}, 'xp': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[layer index: 2] all variables: {'xw', 'xo', 'xz', 'xq', 'xr', 'xs', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] learnable params: {'aq', 'cp', 'as', 'bp', 'br', 'bs', 'bq', 'cq', 'ar', 'ap'}\n",
      "\n",
      "\n",
      "[layer index: 2] dependencies: {'xw': ['xp', 'xq', 'xr', 'xs'], 'xz': ['xp', 'xq', 'xr', 'xs'], 'xo': ['xw', 'xz']}\n",
      "\n",
      "\n",
      "[layer index: 2] expressions dict: {'xw': 'ap*xp+aq*xq+ar*xr+as*xs', 'xz': 'bp*xp+bq*xq+br*xr+bs*xs', 'xo': 'cp*xw+cq*xz'}\n",
      "\n",
      "\n",
      "[layer index: 2] var_to_var_param dict: {'xw': {'xp': 'ap', 'xq': 'aq', 'xr': 'ar', 'xs': 'as'}, 'xz': {'xp': 'bp', 'xq': 'bq', 'xr': 'br', 'xs': 'bs'}, 'xo': {'xw': 'cp', 'xz': 'cq'}}\n",
      "\n",
      "\n",
      "[layer index: 2] node to int labels: {'xp': 0, 'xq': 1, 'xr': 2, 'xs': 3, 'xw': 4, 'xz': 5, 'xo': 6}\n",
      "\n",
      "\n",
      "[layer index: 2] independent vars: {'xr', 'xs', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[layer index: 2] leads_to dictionary: {'xw': {'xo'}, 'xo': set(), 'xz': {'xo'}, 'xq': {'xw', 'xz'}, 'xr': {'xw', 'xz'}, 'xs': {'xw', 'xz'}, 'xp': {'xw', 'xz'}}\n",
      "\n",
      "\n",
      "[Final] independent vars: {'xr', 'xs', 'xq', 'xp'}\n",
      "\n",
      "\n",
      "[Final] self.layer_vars:  {0: ['xp', 'xq', 'xr', 'xs'], 1: ['xw', 'xz'], 2: ['xo']}\n",
      "\n",
      "\n",
      "[Final] self.layer_params:  {1: [['ap', 'aq', 'ar', 'as'], ['bp', 'bq', 'br', 'bs']], 2: [['cp', 'cq']]}\n",
      "\n",
      "\n",
      "[Final] self.layer_exp_objects:  {1: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001D7376DAE80>, <ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001D7376DA280>], 2: [<ComputationalGraphPrimer.ComputationalGraphPrimer.Exp object at 0x000001D7376DA4C0>]}\n"
     ]
    }
   ],
   "source": [
    "cgp2 = Adam(\n",
    "               num_layers = 3,\n",
    "               layers_config = [4,2,1],                         # num of nodes in each layer\n",
    "               expressions = ['xw=ap*xp+aq*xq+ar*xr+as*xs',\n",
    "                              'xz=bp*xp+bq*xq+br*xr+bs*xs',\n",
    "                              'xo=cp*xw+cq*xz'],\n",
    "               output_vars = ['xo'],\n",
    "               dataset_size = 5000,\n",
    "               learning_rate = 5e-3,\n",
    "#               learning_rate = 5 * 1e-2,\n",
    "               training_iterations = 40000,\n",
    "               batch_size = 8,\n",
    "               display_loss_how_often = 100,\n",
    "               debug = True,\n",
    "      )\n",
    "\n",
    "cgp2.parse_multi_layer_expressions()\n",
    "training_data2 = cgp2.gen_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ac1e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=1]  loss = 0.0048\n",
      "[iter=101]  loss = 0.3295\n",
      "[iter=201]  loss = 0.2680\n",
      "[iter=301]  loss = 0.2533\n",
      "[iter=401]  loss = 0.2507\n",
      "[iter=501]  loss = 0.2506\n",
      "[iter=601]  loss = 0.2506\n",
      "[iter=701]  loss = 0.2506\n",
      "[iter=801]  loss = 0.2504\n",
      "[iter=901]  loss = 0.2505\n",
      "[iter=1001]  loss = 0.2506\n",
      "[iter=1101]  loss = 0.2502\n",
      "[iter=1201]  loss = 0.2506\n",
      "[iter=1301]  loss = 0.2507\n",
      "[iter=1401]  loss = 0.2507\n",
      "[iter=1501]  loss = 0.2504\n",
      "[iter=1601]  loss = 0.2497\n",
      "[iter=1701]  loss = 0.2503\n",
      "[iter=1801]  loss = 0.2511\n",
      "[iter=1901]  loss = 0.2501\n",
      "[iter=2001]  loss = 0.2504\n",
      "[iter=2101]  loss = 0.2497\n",
      "[iter=2201]  loss = 0.2509\n",
      "[iter=2301]  loss = 0.2504\n",
      "[iter=2401]  loss = 0.2503\n",
      "[iter=2501]  loss = 0.2502\n",
      "[iter=2601]  loss = 0.2497\n",
      "[iter=2701]  loss = 0.2493\n",
      "[iter=2801]  loss = 0.2508\n",
      "[iter=2901]  loss = 0.2503\n",
      "[iter=3001]  loss = 0.2504\n",
      "[iter=3101]  loss = 0.2503\n",
      "[iter=3201]  loss = 0.2501\n",
      "[iter=3301]  loss = 0.2499\n",
      "[iter=3401]  loss = 0.2500\n",
      "[iter=3501]  loss = 0.2486\n",
      "[iter=3601]  loss = 0.2509\n",
      "[iter=3701]  loss = 0.2497\n",
      "[iter=3801]  loss = 0.2495\n",
      "[iter=3901]  loss = 0.2495\n",
      "[iter=4001]  loss = 0.2501\n",
      "[iter=4101]  loss = 0.2499\n",
      "[iter=4201]  loss = 0.2503\n",
      "[iter=4301]  loss = 0.2490\n",
      "[iter=4401]  loss = 0.2490\n",
      "[iter=4501]  loss = 0.2488\n",
      "[iter=4601]  loss = 0.2489\n",
      "[iter=4701]  loss = 0.2488\n",
      "[iter=4801]  loss = 0.2494\n",
      "[iter=4901]  loss = 0.2491\n",
      "[iter=5001]  loss = 0.2490\n",
      "[iter=5101]  loss = 0.2484\n",
      "[iter=5201]  loss = 0.2489\n",
      "[iter=5301]  loss = 0.2479\n",
      "[iter=5401]  loss = 0.2480\n",
      "[iter=5501]  loss = 0.2474\n",
      "[iter=5601]  loss = 0.2481\n",
      "[iter=5701]  loss = 0.2477\n",
      "[iter=5801]  loss = 0.2474\n",
      "[iter=5901]  loss = 0.2472\n",
      "[iter=6001]  loss = 0.2470\n",
      "[iter=6101]  loss = 0.2467\n",
      "[iter=6201]  loss = 0.2459\n",
      "[iter=6301]  loss = 0.2459\n",
      "[iter=6401]  loss = 0.2454\n",
      "[iter=6501]  loss = 0.2458\n",
      "[iter=6601]  loss = 0.2460\n",
      "[iter=6701]  loss = 0.2454\n",
      "[iter=6801]  loss = 0.2450\n",
      "[iter=6901]  loss = 0.2447\n",
      "[iter=7001]  loss = 0.2436\n",
      "[iter=7101]  loss = 0.2441\n",
      "[iter=7201]  loss = 0.2433\n",
      "[iter=7301]  loss = 0.2432\n",
      "[iter=7401]  loss = 0.2428\n",
      "[iter=7501]  loss = 0.2429\n",
      "[iter=7601]  loss = 0.2418\n",
      "[iter=7701]  loss = 0.2403\n",
      "[iter=7801]  loss = 0.2394\n",
      "[iter=7901]  loss = 0.2414\n",
      "[iter=8001]  loss = 0.2402\n",
      "[iter=8101]  loss = 0.2396\n",
      "[iter=8201]  loss = 0.2387\n",
      "[iter=8301]  loss = 0.2394\n",
      "[iter=8401]  loss = 0.2391\n",
      "[iter=8501]  loss = 0.2375\n",
      "[iter=8601]  loss = 0.2378\n",
      "[iter=8701]  loss = 0.2364\n",
      "[iter=8801]  loss = 0.2356\n",
      "[iter=8901]  loss = 0.2357\n",
      "[iter=9001]  loss = 0.2341\n",
      "[iter=9101]  loss = 0.2344\n",
      "[iter=9201]  loss = 0.2348\n",
      "[iter=9301]  loss = 0.2330\n",
      "[iter=9401]  loss = 0.2331\n",
      "[iter=9501]  loss = 0.2322\n",
      "[iter=9601]  loss = 0.2319\n",
      "[iter=9701]  loss = 0.2321\n",
      "[iter=9801]  loss = 0.2293\n",
      "[iter=9901]  loss = 0.2292\n",
      "[iter=10001]  loss = 0.2274\n",
      "[iter=10101]  loss = 0.2267\n",
      "[iter=10201]  loss = 0.2286\n",
      "[iter=10301]  loss = 0.2272\n",
      "[iter=10401]  loss = 0.2250\n",
      "[iter=10501]  loss = 0.2250\n",
      "[iter=10601]  loss = 0.2248\n",
      "[iter=10701]  loss = 0.2251\n",
      "[iter=10801]  loss = 0.2237\n",
      "[iter=10901]  loss = 0.2230\n",
      "[iter=11001]  loss = 0.2219\n",
      "[iter=11101]  loss = 0.2228\n",
      "[iter=11201]  loss = 0.2201\n",
      "[iter=11301]  loss = 0.2184\n",
      "[iter=11401]  loss = 0.2226\n",
      "[iter=11501]  loss = 0.2190\n",
      "[iter=11601]  loss = 0.2183\n",
      "[iter=11701]  loss = 0.2160\n",
      "[iter=11801]  loss = 0.2186\n",
      "[iter=11901]  loss = 0.2190\n",
      "[iter=12001]  loss = 0.2160\n",
      "[iter=12101]  loss = 0.2191\n",
      "[iter=12201]  loss = 0.2166\n",
      "[iter=12301]  loss = 0.2153\n",
      "[iter=12401]  loss = 0.2161\n",
      "[iter=12501]  loss = 0.2155\n",
      "[iter=12601]  loss = 0.2144\n",
      "[iter=12701]  loss = 0.2152\n",
      "[iter=12801]  loss = 0.2115\n",
      "[iter=12901]  loss = 0.2118\n",
      "[iter=13001]  loss = 0.2122\n",
      "[iter=13101]  loss = 0.2066\n",
      "[iter=13201]  loss = 0.2082\n",
      "[iter=13301]  loss = 0.2067\n",
      "[iter=13401]  loss = 0.2093\n",
      "[iter=13501]  loss = 0.2067\n",
      "[iter=13601]  loss = 0.2078\n",
      "[iter=13701]  loss = 0.2048\n",
      "[iter=13801]  loss = 0.2060\n",
      "[iter=13901]  loss = 0.2044\n",
      "[iter=14001]  loss = 0.2049\n",
      "[iter=14101]  loss = 0.2002\n",
      "[iter=14201]  loss = 0.2052\n",
      "[iter=14301]  loss = 0.2023\n",
      "[iter=14401]  loss = 0.2050\n",
      "[iter=14501]  loss = 0.1985\n",
      "[iter=14601]  loss = 0.2005\n",
      "[iter=14701]  loss = 0.2007\n",
      "[iter=14801]  loss = 0.1980\n",
      "[iter=14901]  loss = 0.1973\n",
      "[iter=15001]  loss = 0.1953\n",
      "[iter=15101]  loss = 0.1982\n",
      "[iter=15201]  loss = 0.1960\n",
      "[iter=15301]  loss = 0.1995\n",
      "[iter=15401]  loss = 0.1967\n",
      "[iter=15501]  loss = 0.1938\n",
      "[iter=15601]  loss = 0.1951\n",
      "[iter=15701]  loss = 0.1920\n",
      "[iter=15801]  loss = 0.1944\n",
      "[iter=15901]  loss = 0.1926\n",
      "[iter=16001]  loss = 0.1962\n",
      "[iter=16101]  loss = 0.1916\n",
      "[iter=16201]  loss = 0.1896\n",
      "[iter=16301]  loss = 0.1891\n",
      "[iter=16401]  loss = 0.1920\n",
      "[iter=16501]  loss = 0.1895\n",
      "[iter=16601]  loss = 0.1904\n",
      "[iter=16701]  loss = 0.1898\n",
      "[iter=16801]  loss = 0.1947\n",
      "[iter=16901]  loss = 0.1896\n",
      "[iter=17001]  loss = 0.1907\n",
      "[iter=17101]  loss = 0.1935\n",
      "[iter=17201]  loss = 0.1894\n",
      "[iter=17301]  loss = 0.1851\n",
      "[iter=17401]  loss = 0.1874\n",
      "[iter=17501]  loss = 0.1912\n",
      "[iter=17601]  loss = 0.1866\n",
      "[iter=17701]  loss = 0.1859\n",
      "[iter=17801]  loss = 0.1834\n",
      "[iter=17901]  loss = 0.1910\n",
      "[iter=18001]  loss = 0.1784\n",
      "[iter=18101]  loss = 0.1832\n",
      "[iter=18201]  loss = 0.1853\n",
      "[iter=18301]  loss = 0.1856\n",
      "[iter=18401]  loss = 0.1864\n",
      "[iter=18501]  loss = 0.1838\n",
      "[iter=18601]  loss = 0.1815\n",
      "[iter=18701]  loss = 0.1816\n",
      "[iter=18801]  loss = 0.1777\n",
      "[iter=18901]  loss = 0.1808\n",
      "[iter=19001]  loss = 0.1794\n",
      "[iter=19101]  loss = 0.1786\n",
      "[iter=19201]  loss = 0.1747\n",
      "[iter=19301]  loss = 0.1819\n",
      "[iter=19401]  loss = 0.1827\n",
      "[iter=19501]  loss = 0.1756\n",
      "[iter=19601]  loss = 0.1802\n",
      "[iter=19701]  loss = 0.1823\n",
      "[iter=19801]  loss = 0.1824\n",
      "[iter=19901]  loss = 0.1792\n",
      "[iter=20001]  loss = 0.1773\n",
      "[iter=20101]  loss = 0.1737\n",
      "[iter=20201]  loss = 0.1748\n",
      "[iter=20301]  loss = 0.1731\n",
      "[iter=20401]  loss = 0.1748\n",
      "[iter=20501]  loss = 0.1711\n",
      "[iter=20601]  loss = 0.1738\n",
      "[iter=20701]  loss = 0.1720\n",
      "[iter=20801]  loss = 0.1728\n",
      "[iter=20901]  loss = 0.1704\n",
      "[iter=21001]  loss = 0.1691\n",
      "[iter=21101]  loss = 0.1688\n",
      "[iter=21201]  loss = 0.1788\n",
      "[iter=21301]  loss = 0.1677\n",
      "[iter=21401]  loss = 0.1677\n",
      "[iter=21501]  loss = 0.1698\n",
      "[iter=21601]  loss = 0.1694\n",
      "[iter=21701]  loss = 0.1699\n",
      "[iter=21801]  loss = 0.1734\n",
      "[iter=21901]  loss = 0.1694\n",
      "[iter=22001]  loss = 0.1705\n",
      "[iter=22101]  loss = 0.1720\n",
      "[iter=22201]  loss = 0.1654\n",
      "[iter=22301]  loss = 0.1684\n",
      "[iter=22401]  loss = 0.1675\n",
      "[iter=22501]  loss = 0.1627\n",
      "[iter=22601]  loss = 0.1731\n",
      "[iter=22701]  loss = 0.1627\n",
      "[iter=22801]  loss = 0.1696\n",
      "[iter=22901]  loss = 0.1671\n",
      "[iter=23001]  loss = 0.1701\n",
      "[iter=23101]  loss = 0.1602\n",
      "[iter=23201]  loss = 0.1672\n",
      "[iter=23301]  loss = 0.1659\n",
      "[iter=23401]  loss = 0.1646\n",
      "[iter=23501]  loss = 0.1618\n",
      "[iter=23601]  loss = 0.1576\n",
      "[iter=23701]  loss = 0.1599\n",
      "[iter=23801]  loss = 0.1570\n",
      "[iter=23901]  loss = 0.1610\n",
      "[iter=24001]  loss = 0.1633\n",
      "[iter=24101]  loss = 0.1615\n",
      "[iter=24201]  loss = 0.1530\n",
      "[iter=24301]  loss = 0.1620\n",
      "[iter=24401]  loss = 0.1618\n",
      "[iter=24501]  loss = 0.1567\n",
      "[iter=24601]  loss = 0.1603\n",
      "[iter=24701]  loss = 0.1633\n",
      "[iter=24801]  loss = 0.1635\n",
      "[iter=24901]  loss = 0.1567\n",
      "[iter=25001]  loss = 0.1612\n",
      "[iter=25101]  loss = 0.1571\n",
      "[iter=25201]  loss = 0.1601\n",
      "[iter=25301]  loss = 0.1579\n",
      "[iter=25401]  loss = 0.1553\n",
      "[iter=25501]  loss = 0.1569\n",
      "[iter=25601]  loss = 0.1568\n",
      "[iter=25701]  loss = 0.1536\n",
      "[iter=25801]  loss = 0.1660\n",
      "[iter=25901]  loss = 0.1617\n",
      "[iter=26001]  loss = 0.1563\n",
      "[iter=26101]  loss = 0.1571\n",
      "[iter=26201]  loss = 0.1514\n",
      "[iter=26301]  loss = 0.1453\n",
      "[iter=26401]  loss = 0.1473\n",
      "[iter=26501]  loss = 0.1620\n",
      "[iter=26601]  loss = 0.1563\n",
      "[iter=26701]  loss = 0.1523\n",
      "[iter=26801]  loss = 0.1529\n",
      "[iter=26901]  loss = 0.1511\n",
      "[iter=27001]  loss = 0.1568\n",
      "[iter=27101]  loss = 0.1513\n",
      "[iter=27201]  loss = 0.1535\n",
      "[iter=27301]  loss = 0.1507\n",
      "[iter=27401]  loss = 0.1579\n",
      "[iter=27501]  loss = 0.1532\n",
      "[iter=27601]  loss = 0.1530\n",
      "[iter=27701]  loss = 0.1597\n",
      "[iter=27801]  loss = 0.1475\n",
      "[iter=27901]  loss = 0.1610\n",
      "[iter=28001]  loss = 0.1493\n",
      "[iter=28101]  loss = 0.1552\n",
      "[iter=28201]  loss = 0.1553\n",
      "[iter=28301]  loss = 0.1518\n",
      "[iter=28401]  loss = 0.1476\n",
      "[iter=28501]  loss = 0.1516\n",
      "[iter=28601]  loss = 0.1502\n",
      "[iter=28701]  loss = 0.1497\n",
      "[iter=28801]  loss = 0.1495\n",
      "[iter=28901]  loss = 0.1576\n",
      "[iter=29001]  loss = 0.1571\n",
      "[iter=29101]  loss = 0.1538\n",
      "[iter=29201]  loss = 0.1510\n",
      "[iter=29301]  loss = 0.1526\n",
      "[iter=29401]  loss = 0.1522\n",
      "[iter=29501]  loss = 0.1501\n",
      "[iter=29601]  loss = 0.1465\n",
      "[iter=29701]  loss = 0.1452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=29801]  loss = 0.1464\n",
      "[iter=29901]  loss = 0.1471\n",
      "[iter=30001]  loss = 0.1513\n",
      "[iter=30101]  loss = 0.1498\n",
      "[iter=30201]  loss = 0.1530\n",
      "[iter=30301]  loss = 0.1471\n",
      "[iter=30401]  loss = 0.1448\n",
      "[iter=30501]  loss = 0.1443\n",
      "[iter=30601]  loss = 0.1514\n",
      "[iter=30701]  loss = 0.1515\n",
      "[iter=30801]  loss = 0.1481\n",
      "[iter=30901]  loss = 0.1575\n",
      "[iter=31001]  loss = 0.1482\n",
      "[iter=31101]  loss = 0.1433\n",
      "[iter=31201]  loss = 0.1442\n",
      "[iter=31301]  loss = 0.1381\n",
      "[iter=31401]  loss = 0.1477\n",
      "[iter=31501]  loss = 0.1474\n",
      "[iter=31601]  loss = 0.1425\n",
      "[iter=31701]  loss = 0.1440\n",
      "[iter=31801]  loss = 0.1413\n",
      "[iter=31901]  loss = 0.1495\n",
      "[iter=32001]  loss = 0.1435\n",
      "[iter=32101]  loss = 0.1440\n",
      "[iter=32201]  loss = 0.1426\n",
      "[iter=32301]  loss = 0.1394\n",
      "[iter=32401]  loss = 0.1478\n",
      "[iter=32501]  loss = 0.1568\n",
      "[iter=32601]  loss = 0.1510\n",
      "[iter=32701]  loss = 0.1415\n",
      "[iter=32801]  loss = 0.1393\n",
      "[iter=32901]  loss = 0.1463\n",
      "[iter=33001]  loss = 0.1448\n",
      "[iter=33101]  loss = 0.1434\n",
      "[iter=33201]  loss = 0.1461\n",
      "[iter=33301]  loss = 0.1395\n",
      "[iter=33401]  loss = 0.1459\n",
      "[iter=33501]  loss = 0.1485\n",
      "[iter=33601]  loss = 0.1466\n",
      "[iter=33701]  loss = 0.1428\n",
      "[iter=33801]  loss = 0.1423\n",
      "[iter=33901]  loss = 0.1399\n",
      "[iter=34001]  loss = 0.1390\n",
      "[iter=34101]  loss = 0.1442\n",
      "[iter=34201]  loss = 0.1403\n",
      "[iter=34301]  loss = 0.1448\n",
      "[iter=34401]  loss = 0.1516\n",
      "[iter=34501]  loss = 0.1438\n",
      "[iter=34601]  loss = 0.1424\n",
      "[iter=34701]  loss = 0.1492\n",
      "[iter=34801]  loss = 0.1477\n",
      "[iter=34901]  loss = 0.1429\n",
      "[iter=35001]  loss = 0.1514\n",
      "[iter=35101]  loss = 0.1404\n",
      "[iter=35201]  loss = 0.1477\n",
      "[iter=35301]  loss = 0.1476\n",
      "[iter=35401]  loss = 0.1403\n",
      "[iter=35501]  loss = 0.1431\n",
      "[iter=35601]  loss = 0.1463\n",
      "[iter=35701]  loss = 0.1503\n",
      "[iter=35801]  loss = 0.1447\n",
      "[iter=35901]  loss = 0.1498\n",
      "[iter=36001]  loss = 0.1415\n",
      "[iter=36101]  loss = 0.1414\n",
      "[iter=36201]  loss = 0.1399\n",
      "[iter=36301]  loss = 0.1456\n",
      "[iter=36401]  loss = 0.1380\n",
      "[iter=36501]  loss = 0.1445\n",
      "[iter=36601]  loss = 0.1412\n",
      "[iter=36701]  loss = 0.1586\n",
      "[iter=36801]  loss = 0.1417\n",
      "[iter=36901]  loss = 0.1415\n",
      "[iter=37001]  loss = 0.1369\n",
      "[iter=37101]  loss = 0.1423\n",
      "[iter=37201]  loss = 0.1422\n",
      "[iter=37301]  loss = 0.1386\n",
      "[iter=37401]  loss = 0.1437\n",
      "[iter=37501]  loss = 0.1412\n",
      "[iter=37601]  loss = 0.1382\n",
      "[iter=37701]  loss = 0.1433\n",
      "[iter=37801]  loss = 0.1397\n",
      "[iter=37901]  loss = 0.1474\n",
      "[iter=38001]  loss = 0.1401\n",
      "[iter=38101]  loss = 0.1421\n",
      "[iter=38201]  loss = 0.1369\n",
      "[iter=38301]  loss = 0.1369\n",
      "[iter=38401]  loss = 0.1377\n",
      "[iter=38501]  loss = 0.1382\n",
      "[iter=38601]  loss = 0.1302\n",
      "[iter=38701]  loss = 0.1403\n",
      "[iter=38801]  loss = 0.1363\n",
      "[iter=38901]  loss = 0.1495\n",
      "[iter=39001]  loss = 0.1471\n",
      "[iter=39101]  loss = 0.1414\n",
      "[iter=39201]  loss = 0.1456\n",
      "[iter=39301]  loss = 0.1437\n",
      "[iter=39401]  loss = 0.1380\n",
      "[iter=39501]  loss = 0.1352\n",
      "[iter=39601]  loss = 0.1397\n",
      "[iter=39701]  loss = 0.1337\n",
      "[iter=39801]  loss = 0.1343\n",
      "[iter=39901]  loss = 0.1407\n",
      "[iter=1]  loss = 0.0027\n",
      "[iter=101]  loss = 0.3369\n",
      "[iter=201]  loss = 0.3574\n",
      "[iter=301]  loss = 0.3193\n",
      "[iter=401]  loss = 0.3215\n",
      "[iter=501]  loss = 0.3172\n",
      "[iter=601]  loss = 0.3313\n",
      "[iter=701]  loss = 0.3254\n",
      "[iter=801]  loss = 0.3204\n",
      "[iter=901]  loss = 0.2999\n",
      "[iter=1001]  loss = 0.2942\n",
      "[iter=1101]  loss = 0.3047\n",
      "[iter=1201]  loss = 0.2905\n",
      "[iter=1301]  loss = 0.2781\n",
      "[iter=1401]  loss = 0.2821\n",
      "[iter=1501]  loss = 0.2892\n",
      "[iter=1601]  loss = 0.2803\n",
      "[iter=1701]  loss = 0.2776\n",
      "[iter=1801]  loss = 0.2715\n",
      "[iter=1901]  loss = 0.2824\n",
      "[iter=2001]  loss = 0.2774\n",
      "[iter=2101]  loss = 0.2722\n",
      "[iter=2201]  loss = 0.2736\n",
      "[iter=2301]  loss = 0.2747\n",
      "[iter=2401]  loss = 0.2646\n",
      "[iter=2501]  loss = 0.2591\n",
      "[iter=2601]  loss = 0.2619\n",
      "[iter=2701]  loss = 0.2664\n",
      "[iter=2801]  loss = 0.2649\n",
      "[iter=2901]  loss = 0.2642\n",
      "[iter=3001]  loss = 0.2568\n",
      "[iter=3101]  loss = 0.2566\n",
      "[iter=3201]  loss = 0.2552\n",
      "[iter=3301]  loss = 0.2515\n",
      "[iter=3401]  loss = 0.2486\n",
      "[iter=3501]  loss = 0.2599\n",
      "[iter=3601]  loss = 0.2577\n",
      "[iter=3701]  loss = 0.2528\n",
      "[iter=3801]  loss = 0.2573\n",
      "[iter=3901]  loss = 0.2555\n",
      "[iter=4001]  loss = 0.2545\n",
      "[iter=4101]  loss = 0.2524\n",
      "[iter=4201]  loss = 0.2537\n",
      "[iter=4301]  loss = 0.2543\n",
      "[iter=4401]  loss = 0.2544\n",
      "[iter=4501]  loss = 0.2512\n",
      "[iter=4601]  loss = 0.2536\n",
      "[iter=4701]  loss = 0.2502\n",
      "[iter=4801]  loss = 0.2539\n",
      "[iter=4901]  loss = 0.2512\n",
      "[iter=5001]  loss = 0.2517\n",
      "[iter=5101]  loss = 0.2519\n",
      "[iter=5201]  loss = 0.2521\n",
      "[iter=5301]  loss = 0.2506\n",
      "[iter=5401]  loss = 0.2518\n",
      "[iter=5501]  loss = 0.2478\n",
      "[iter=5601]  loss = 0.2518\n",
      "[iter=5701]  loss = 0.2530\n",
      "[iter=5801]  loss = 0.2507\n",
      "[iter=5901]  loss = 0.2499\n",
      "[iter=6001]  loss = 0.2517\n",
      "[iter=6101]  loss = 0.2505\n",
      "[iter=6201]  loss = 0.2518\n",
      "[iter=6301]  loss = 0.2501\n",
      "[iter=6401]  loss = 0.2496\n",
      "[iter=6501]  loss = 0.2519\n",
      "[iter=6601]  loss = 0.2512\n",
      "[iter=6701]  loss = 0.2517\n",
      "[iter=6801]  loss = 0.2501\n",
      "[iter=6901]  loss = 0.2508\n",
      "[iter=7001]  loss = 0.2503\n",
      "[iter=7101]  loss = 0.2521\n",
      "[iter=7201]  loss = 0.2506\n",
      "[iter=7301]  loss = 0.2510\n",
      "[iter=7401]  loss = 0.2511\n",
      "[iter=7501]  loss = 0.2511\n",
      "[iter=7601]  loss = 0.2510\n",
      "[iter=7701]  loss = 0.2512\n",
      "[iter=7801]  loss = 0.2508\n",
      "[iter=7901]  loss = 0.2505\n",
      "[iter=8001]  loss = 0.2513\n",
      "[iter=8101]  loss = 0.2507\n",
      "[iter=8201]  loss = 0.2510\n",
      "[iter=8301]  loss = 0.2508\n",
      "[iter=8401]  loss = 0.2510\n",
      "[iter=8501]  loss = 0.2504\n",
      "[iter=8601]  loss = 0.2512\n",
      "[iter=8701]  loss = 0.2504\n",
      "[iter=8801]  loss = 0.2506\n",
      "[iter=8901]  loss = 0.2511\n",
      "[iter=9001]  loss = 0.2508\n",
      "[iter=9101]  loss = 0.2506\n",
      "[iter=9201]  loss = 0.2515\n",
      "[iter=9301]  loss = 0.2512\n",
      "[iter=9401]  loss = 0.2508\n",
      "[iter=9501]  loss = 0.2507\n",
      "[iter=9601]  loss = 0.2507\n",
      "[iter=9701]  loss = 0.2507\n",
      "[iter=9801]  loss = 0.2509\n",
      "[iter=9901]  loss = 0.2507\n",
      "[iter=10001]  loss = 0.2509\n",
      "[iter=10101]  loss = 0.2511\n",
      "[iter=10201]  loss = 0.2513\n",
      "[iter=10301]  loss = 0.2513\n",
      "[iter=10401]  loss = 0.2511\n",
      "[iter=10501]  loss = 0.2509\n",
      "[iter=10601]  loss = 0.2508\n",
      "[iter=10701]  loss = 0.2510\n",
      "[iter=10801]  loss = 0.2507\n",
      "[iter=10901]  loss = 0.2508\n",
      "[iter=11001]  loss = 0.2509\n",
      "[iter=11101]  loss = 0.2507\n",
      "[iter=11201]  loss = 0.2510\n",
      "[iter=11301]  loss = 0.2511\n",
      "[iter=11401]  loss = 0.2510\n",
      "[iter=11501]  loss = 0.2510\n",
      "[iter=11601]  loss = 0.2510\n",
      "[iter=11701]  loss = 0.2509\n",
      "[iter=11801]  loss = 0.2507\n",
      "[iter=11901]  loss = 0.2507\n",
      "[iter=12001]  loss = 0.2507\n",
      "[iter=12101]  loss = 0.2507\n",
      "[iter=12201]  loss = 0.2507\n",
      "[iter=12301]  loss = 0.2506\n",
      "[iter=12401]  loss = 0.2508\n",
      "[iter=12501]  loss = 0.2512\n",
      "[iter=12601]  loss = 0.2509\n",
      "[iter=12701]  loss = 0.2504\n",
      "[iter=12801]  loss = 0.2509\n",
      "[iter=12901]  loss = 0.2507\n",
      "[iter=13001]  loss = 0.2507\n",
      "[iter=13101]  loss = 0.2509\n",
      "[iter=13201]  loss = 0.2508\n",
      "[iter=13301]  loss = 0.2508\n",
      "[iter=13401]  loss = 0.2509\n",
      "[iter=13501]  loss = 0.2507\n",
      "[iter=13601]  loss = 0.2509\n",
      "[iter=13701]  loss = 0.2510\n",
      "[iter=13801]  loss = 0.2510\n",
      "[iter=13901]  loss = 0.2509\n",
      "[iter=14001]  loss = 0.2505\n",
      "[iter=14101]  loss = 0.2507\n",
      "[iter=14201]  loss = 0.2508\n",
      "[iter=14301]  loss = 0.2507\n",
      "[iter=14401]  loss = 0.2507\n",
      "[iter=14501]  loss = 0.2508\n",
      "[iter=14601]  loss = 0.2509\n",
      "[iter=14701]  loss = 0.2508\n",
      "[iter=14801]  loss = 0.2503\n",
      "[iter=14901]  loss = 0.2508\n",
      "[iter=15001]  loss = 0.2505\n",
      "[iter=15101]  loss = 0.2506\n",
      "[iter=15201]  loss = 0.2506\n",
      "[iter=15301]  loss = 0.2507\n",
      "[iter=15401]  loss = 0.2506\n",
      "[iter=15501]  loss = 0.2506\n",
      "[iter=15601]  loss = 0.2508\n",
      "[iter=15701]  loss = 0.2509\n",
      "[iter=15801]  loss = 0.2504\n",
      "[iter=15901]  loss = 0.2505\n",
      "[iter=16001]  loss = 0.2506\n",
      "[iter=16101]  loss = 0.2505\n",
      "[iter=16201]  loss = 0.2504\n",
      "[iter=16301]  loss = 0.2508\n",
      "[iter=16401]  loss = 0.2503\n",
      "[iter=16501]  loss = 0.2503\n",
      "[iter=16601]  loss = 0.2508\n",
      "[iter=16701]  loss = 0.2511\n",
      "[iter=16801]  loss = 0.2507\n",
      "[iter=16901]  loss = 0.2505\n",
      "[iter=17001]  loss = 0.2510\n",
      "[iter=17101]  loss = 0.2503\n",
      "[iter=17201]  loss = 0.2504\n",
      "[iter=17301]  loss = 0.2507\n",
      "[iter=17401]  loss = 0.2503\n",
      "[iter=17501]  loss = 0.2504\n",
      "[iter=17601]  loss = 0.2505\n",
      "[iter=17701]  loss = 0.2501\n",
      "[iter=17801]  loss = 0.2504\n",
      "[iter=17901]  loss = 0.2502\n",
      "[iter=18001]  loss = 0.2503\n",
      "[iter=18101]  loss = 0.2503\n",
      "[iter=18201]  loss = 0.2504\n",
      "[iter=18301]  loss = 0.2504\n",
      "[iter=18401]  loss = 0.2503\n",
      "[iter=18501]  loss = 0.2504\n",
      "[iter=18601]  loss = 0.2506\n",
      "[iter=18701]  loss = 0.2506\n",
      "[iter=18801]  loss = 0.2504\n",
      "[iter=18901]  loss = 0.2499\n",
      "[iter=19001]  loss = 0.2504\n",
      "[iter=19101]  loss = 0.2505\n",
      "[iter=19201]  loss = 0.2505\n",
      "[iter=19301]  loss = 0.2507\n",
      "[iter=19401]  loss = 0.2505\n",
      "[iter=19501]  loss = 0.2506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=19601]  loss = 0.2504\n",
      "[iter=19701]  loss = 0.2501\n",
      "[iter=19801]  loss = 0.2500\n",
      "[iter=19901]  loss = 0.2501\n",
      "[iter=20001]  loss = 0.2504\n",
      "[iter=20101]  loss = 0.2502\n",
      "[iter=20201]  loss = 0.2502\n",
      "[iter=20301]  loss = 0.2500\n",
      "[iter=20401]  loss = 0.2505\n",
      "[iter=20501]  loss = 0.2501\n",
      "[iter=20601]  loss = 0.2501\n",
      "[iter=20701]  loss = 0.2503\n",
      "[iter=20801]  loss = 0.2501\n",
      "[iter=20901]  loss = 0.2501\n",
      "[iter=21001]  loss = 0.2503\n",
      "[iter=21101]  loss = 0.2498\n",
      "[iter=21201]  loss = 0.2504\n",
      "[iter=21301]  loss = 0.2503\n",
      "[iter=21401]  loss = 0.2499\n",
      "[iter=21501]  loss = 0.2498\n",
      "[iter=21601]  loss = 0.2500\n",
      "[iter=21701]  loss = 0.2503\n",
      "[iter=21801]  loss = 0.2495\n",
      "[iter=21901]  loss = 0.2501\n",
      "[iter=22001]  loss = 0.2500\n",
      "[iter=22101]  loss = 0.2506\n",
      "[iter=22201]  loss = 0.2503\n",
      "[iter=22301]  loss = 0.2501\n",
      "[iter=22401]  loss = 0.2500\n",
      "[iter=22501]  loss = 0.2499\n",
      "[iter=22601]  loss = 0.2500\n",
      "[iter=22701]  loss = 0.2499\n",
      "[iter=22801]  loss = 0.2499\n",
      "[iter=22901]  loss = 0.2505\n",
      "[iter=23001]  loss = 0.2500\n",
      "[iter=23101]  loss = 0.2501\n",
      "[iter=23201]  loss = 0.2497\n",
      "[iter=23301]  loss = 0.2500\n",
      "[iter=23401]  loss = 0.2503\n",
      "[iter=23501]  loss = 0.2500\n",
      "[iter=23601]  loss = 0.2499\n",
      "[iter=23701]  loss = 0.2497\n",
      "[iter=23801]  loss = 0.2499\n",
      "[iter=23901]  loss = 0.2501\n",
      "[iter=24001]  loss = 0.2502\n",
      "[iter=24101]  loss = 0.2501\n",
      "[iter=24201]  loss = 0.2502\n",
      "[iter=24301]  loss = 0.2494\n",
      "[iter=24401]  loss = 0.2499\n",
      "[iter=24501]  loss = 0.2500\n",
      "[iter=24601]  loss = 0.2500\n",
      "[iter=24701]  loss = 0.2498\n",
      "[iter=24801]  loss = 0.2497\n",
      "[iter=24901]  loss = 0.2499\n",
      "[iter=25001]  loss = 0.2500\n",
      "[iter=25101]  loss = 0.2504\n",
      "[iter=25201]  loss = 0.2502\n",
      "[iter=25301]  loss = 0.2500\n",
      "[iter=25401]  loss = 0.2502\n",
      "[iter=25501]  loss = 0.2501\n",
      "[iter=25601]  loss = 0.2500\n",
      "[iter=25701]  loss = 0.2500\n",
      "[iter=25801]  loss = 0.2500\n",
      "[iter=25901]  loss = 0.2499\n",
      "[iter=26001]  loss = 0.2498\n",
      "[iter=26101]  loss = 0.2501\n",
      "[iter=26201]  loss = 0.2501\n",
      "[iter=26301]  loss = 0.2500\n",
      "[iter=26401]  loss = 0.2498\n",
      "[iter=26501]  loss = 0.2499\n",
      "[iter=26601]  loss = 0.2499\n",
      "[iter=26701]  loss = 0.2496\n",
      "[iter=26801]  loss = 0.2495\n",
      "[iter=26901]  loss = 0.2500\n",
      "[iter=27001]  loss = 0.2499\n",
      "[iter=27101]  loss = 0.2500\n",
      "[iter=27201]  loss = 0.2500\n",
      "[iter=27301]  loss = 0.2497\n",
      "[iter=27401]  loss = 0.2498\n",
      "[iter=27501]  loss = 0.2505\n",
      "[iter=27601]  loss = 0.2499\n",
      "[iter=27701]  loss = 0.2499\n",
      "[iter=27801]  loss = 0.2497\n",
      "[iter=27901]  loss = 0.2498\n",
      "[iter=28001]  loss = 0.2499\n",
      "[iter=28101]  loss = 0.2498\n",
      "[iter=28201]  loss = 0.2498\n",
      "[iter=28301]  loss = 0.2498\n",
      "[iter=28401]  loss = 0.2499\n",
      "[iter=28501]  loss = 0.2500\n",
      "[iter=28601]  loss = 0.2497\n",
      "[iter=28701]  loss = 0.2497\n",
      "[iter=28801]  loss = 0.2499\n",
      "[iter=28901]  loss = 0.2498\n",
      "[iter=29001]  loss = 0.2498\n",
      "[iter=29101]  loss = 0.2497\n",
      "[iter=29201]  loss = 0.2499\n",
      "[iter=29301]  loss = 0.2500\n",
      "[iter=29401]  loss = 0.2498\n",
      "[iter=29501]  loss = 0.2501\n",
      "[iter=29601]  loss = 0.2501\n",
      "[iter=29701]  loss = 0.2499\n",
      "[iter=29801]  loss = 0.2500\n",
      "[iter=29901]  loss = 0.2497\n",
      "[iter=30001]  loss = 0.2498\n",
      "[iter=30101]  loss = 0.2497\n",
      "[iter=30201]  loss = 0.2497\n",
      "[iter=30301]  loss = 0.2500\n",
      "[iter=30401]  loss = 0.2500\n",
      "[iter=30501]  loss = 0.2495\n",
      "[iter=30601]  loss = 0.2499\n",
      "[iter=30701]  loss = 0.2497\n",
      "[iter=30801]  loss = 0.2498\n",
      "[iter=30901]  loss = 0.2496\n",
      "[iter=31001]  loss = 0.2497\n",
      "[iter=31101]  loss = 0.2500\n",
      "[iter=31201]  loss = 0.2492\n",
      "[iter=31301]  loss = 0.2494\n",
      "[iter=31401]  loss = 0.2498\n",
      "[iter=31501]  loss = 0.2496\n",
      "[iter=31601]  loss = 0.2495\n",
      "[iter=31701]  loss = 0.2498\n",
      "[iter=31801]  loss = 0.2498\n",
      "[iter=31901]  loss = 0.2494\n",
      "[iter=32001]  loss = 0.2494\n",
      "[iter=32101]  loss = 0.2498\n",
      "[iter=32201]  loss = 0.2496\n",
      "[iter=32301]  loss = 0.2498\n",
      "[iter=32401]  loss = 0.2498\n",
      "[iter=32501]  loss = 0.2496\n",
      "[iter=32601]  loss = 0.2493\n",
      "[iter=32701]  loss = 0.2501\n",
      "[iter=32801]  loss = 0.2495\n",
      "[iter=32901]  loss = 0.2499\n",
      "[iter=33001]  loss = 0.2497\n",
      "[iter=33101]  loss = 0.2494\n",
      "[iter=33201]  loss = 0.2493\n",
      "[iter=33301]  loss = 0.2494\n",
      "[iter=33401]  loss = 0.2493\n",
      "[iter=33501]  loss = 0.2496\n",
      "[iter=33601]  loss = 0.2493\n",
      "[iter=33701]  loss = 0.2494\n",
      "[iter=33801]  loss = 0.2493\n",
      "[iter=33901]  loss = 0.2492\n",
      "[iter=34001]  loss = 0.2496\n",
      "[iter=34101]  loss = 0.2494\n",
      "[iter=34201]  loss = 0.2493\n",
      "[iter=34301]  loss = 0.2494\n",
      "[iter=34401]  loss = 0.2493\n",
      "[iter=34501]  loss = 0.2494\n",
      "[iter=34601]  loss = 0.2496\n",
      "[iter=34701]  loss = 0.2493\n",
      "[iter=34801]  loss = 0.2492\n",
      "[iter=34901]  loss = 0.2493\n",
      "[iter=35001]  loss = 0.2496\n",
      "[iter=35101]  loss = 0.2491\n",
      "[iter=35201]  loss = 0.2493\n",
      "[iter=35301]  loss = 0.2494\n",
      "[iter=35401]  loss = 0.2495\n",
      "[iter=35501]  loss = 0.2490\n",
      "[iter=35601]  loss = 0.2491\n",
      "[iter=35701]  loss = 0.2491\n",
      "[iter=35801]  loss = 0.2486\n",
      "[iter=35901]  loss = 0.2496\n",
      "[iter=36001]  loss = 0.2494\n",
      "[iter=36101]  loss = 0.2491\n",
      "[iter=36201]  loss = 0.2493\n",
      "[iter=36301]  loss = 0.2493\n",
      "[iter=36401]  loss = 0.2494\n",
      "[iter=36501]  loss = 0.2493\n",
      "[iter=36601]  loss = 0.2489\n",
      "[iter=36701]  loss = 0.2491\n",
      "[iter=36801]  loss = 0.2491\n",
      "[iter=36901]  loss = 0.2495\n",
      "[iter=37001]  loss = 0.2495\n",
      "[iter=37101]  loss = 0.2494\n",
      "[iter=37201]  loss = 0.2493\n",
      "[iter=37301]  loss = 0.2493\n",
      "[iter=37401]  loss = 0.2488\n",
      "[iter=37501]  loss = 0.2494\n",
      "[iter=37601]  loss = 0.2492\n",
      "[iter=37701]  loss = 0.2492\n",
      "[iter=37801]  loss = 0.2494\n",
      "[iter=37901]  loss = 0.2494\n",
      "[iter=38001]  loss = 0.2493\n",
      "[iter=38101]  loss = 0.2493\n",
      "[iter=38201]  loss = 0.2492\n",
      "[iter=38301]  loss = 0.2491\n",
      "[iter=38401]  loss = 0.2490\n",
      "[iter=38501]  loss = 0.2491\n",
      "[iter=38601]  loss = 0.2493\n",
      "[iter=38701]  loss = 0.2496\n",
      "[iter=38801]  loss = 0.2491\n",
      "[iter=38901]  loss = 0.2492\n",
      "[iter=39001]  loss = 0.2492\n",
      "[iter=39101]  loss = 0.2491\n",
      "[iter=39201]  loss = 0.2490\n",
      "[iter=39301]  loss = 0.2488\n",
      "[iter=39401]  loss = 0.2490\n",
      "[iter=39501]  loss = 0.2488\n",
      "[iter=39601]  loss = 0.2491\n",
      "[iter=39701]  loss = 0.2490\n",
      "[iter=39801]  loss = 0.2491\n",
      "[iter=39901]  loss = 0.2490\n",
      "[iter=1]  loss = 0.0037\n",
      "[iter=101]  loss = 0.2865\n",
      "[iter=201]  loss = 0.2777\n",
      "[iter=301]  loss = 0.2564\n",
      "[iter=401]  loss = 0.2527\n",
      "[iter=501]  loss = 0.2512\n",
      "[iter=601]  loss = 0.2509\n",
      "[iter=701]  loss = 0.2494\n",
      "[iter=801]  loss = 0.2494\n",
      "[iter=901]  loss = 0.2492\n",
      "[iter=1001]  loss = 0.2492\n",
      "[iter=1101]  loss = 0.2495\n",
      "[iter=1201]  loss = 0.2495\n",
      "[iter=1301]  loss = 0.2491\n",
      "[iter=1401]  loss = 0.2495\n",
      "[iter=1501]  loss = 0.2494\n",
      "[iter=1601]  loss = 0.2492\n",
      "[iter=1701]  loss = 0.2497\n",
      "[iter=1801]  loss = 0.2494\n",
      "[iter=1901]  loss = 0.2494\n",
      "[iter=2001]  loss = 0.2488\n",
      "[iter=2101]  loss = 0.2494\n",
      "[iter=2201]  loss = 0.2491\n",
      "[iter=2301]  loss = 0.2495\n",
      "[iter=2401]  loss = 0.2491\n",
      "[iter=2501]  loss = 0.2497\n",
      "[iter=2601]  loss = 0.2495\n",
      "[iter=2701]  loss = 0.2492\n",
      "[iter=2801]  loss = 0.2493\n",
      "[iter=2901]  loss = 0.2492\n",
      "[iter=3001]  loss = 0.2488\n",
      "[iter=3101]  loss = 0.2491\n",
      "[iter=3201]  loss = 0.2489\n",
      "[iter=3301]  loss = 0.2486\n",
      "[iter=3401]  loss = 0.2490\n",
      "[iter=3501]  loss = 0.2490\n",
      "[iter=3601]  loss = 0.2491\n",
      "[iter=3701]  loss = 0.2490\n",
      "[iter=3801]  loss = 0.2491\n",
      "[iter=3901]  loss = 0.2493\n",
      "[iter=4001]  loss = 0.2491\n",
      "[iter=4101]  loss = 0.2490\n",
      "[iter=4201]  loss = 0.2492\n",
      "[iter=4301]  loss = 0.2491\n",
      "[iter=4401]  loss = 0.2490\n",
      "[iter=4501]  loss = 0.2490\n",
      "[iter=4601]  loss = 0.2489\n",
      "[iter=4701]  loss = 0.2493\n",
      "[iter=4801]  loss = 0.2489\n",
      "[iter=4901]  loss = 0.2490\n",
      "[iter=5001]  loss = 0.2488\n",
      "[iter=5101]  loss = 0.2487\n",
      "[iter=5201]  loss = 0.2489\n",
      "[iter=5301]  loss = 0.2489\n",
      "[iter=5401]  loss = 0.2489\n",
      "[iter=5501]  loss = 0.2489\n",
      "[iter=5601]  loss = 0.2486\n",
      "[iter=5701]  loss = 0.2488\n",
      "[iter=5801]  loss = 0.2482\n",
      "[iter=5901]  loss = 0.2487\n",
      "[iter=6001]  loss = 0.2492\n",
      "[iter=6101]  loss = 0.2490\n",
      "[iter=6201]  loss = 0.2487\n",
      "[iter=6301]  loss = 0.2489\n",
      "[iter=6401]  loss = 0.2488\n",
      "[iter=6501]  loss = 0.2486\n",
      "[iter=6601]  loss = 0.2487\n",
      "[iter=6701]  loss = 0.2484\n",
      "[iter=6801]  loss = 0.2487\n",
      "[iter=6901]  loss = 0.2487\n",
      "[iter=7001]  loss = 0.2486\n",
      "[iter=7101]  loss = 0.2485\n",
      "[iter=7201]  loss = 0.2485\n",
      "[iter=7301]  loss = 0.2488\n",
      "[iter=7401]  loss = 0.2485\n",
      "[iter=7501]  loss = 0.2482\n",
      "[iter=7601]  loss = 0.2480\n",
      "[iter=7701]  loss = 0.2460\n",
      "[iter=7801]  loss = 0.2497\n",
      "[iter=7901]  loss = 0.2487\n",
      "[iter=8001]  loss = 0.2490\n",
      "[iter=8101]  loss = 0.2485\n",
      "[iter=8201]  loss = 0.2484\n",
      "[iter=8301]  loss = 0.2488\n",
      "[iter=8401]  loss = 0.2482\n",
      "[iter=8501]  loss = 0.2483\n",
      "[iter=8601]  loss = 0.2476\n",
      "[iter=8701]  loss = 0.2479\n",
      "[iter=8801]  loss = 0.2486\n",
      "[iter=8901]  loss = 0.2484\n",
      "[iter=9001]  loss = 0.2482\n",
      "[iter=9101]  loss = 0.2484\n",
      "[iter=9201]  loss = 0.2482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=9301]  loss = 0.2481\n",
      "[iter=9401]  loss = 0.2477\n",
      "[iter=9501]  loss = 0.2482\n",
      "[iter=9601]  loss = 0.2480\n",
      "[iter=9701]  loss = 0.2475\n",
      "[iter=9801]  loss = 0.2474\n",
      "[iter=9901]  loss = 0.2483\n",
      "[iter=10001]  loss = 0.2480\n",
      "[iter=10101]  loss = 0.2478\n",
      "[iter=10201]  loss = 0.2473\n",
      "[iter=10301]  loss = 0.2480\n",
      "[iter=10401]  loss = 0.2480\n",
      "[iter=10501]  loss = 0.2480\n",
      "[iter=10601]  loss = 0.2479\n",
      "[iter=10701]  loss = 0.2480\n",
      "[iter=10801]  loss = 0.2477\n",
      "[iter=10901]  loss = 0.2486\n",
      "[iter=11001]  loss = 0.2477\n",
      "[iter=11101]  loss = 0.2478\n",
      "[iter=11201]  loss = 0.2476\n",
      "[iter=11301]  loss = 0.2477\n",
      "[iter=11401]  loss = 0.2483\n",
      "[iter=11501]  loss = 0.2475\n",
      "[iter=11601]  loss = 0.2480\n",
      "[iter=11701]  loss = 0.2477\n",
      "[iter=11801]  loss = 0.2478\n",
      "[iter=11901]  loss = 0.2476\n",
      "[iter=12001]  loss = 0.2475\n",
      "[iter=12101]  loss = 0.2476\n",
      "[iter=12201]  loss = 0.2475\n",
      "[iter=12301]  loss = 0.2470\n",
      "[iter=12401]  loss = 0.2476\n",
      "[iter=12501]  loss = 0.2471\n",
      "[iter=12601]  loss = 0.2477\n",
      "[iter=12701]  loss = 0.2475\n",
      "[iter=12801]  loss = 0.2476\n",
      "[iter=12901]  loss = 0.2477\n",
      "[iter=13001]  loss = 0.2474\n",
      "[iter=13101]  loss = 0.2475\n",
      "[iter=13201]  loss = 0.2474\n",
      "[iter=13301]  loss = 0.2472\n",
      "[iter=13401]  loss = 0.2474\n",
      "[iter=13501]  loss = 0.2471\n",
      "[iter=13601]  loss = 0.2477\n",
      "[iter=13701]  loss = 0.2473\n",
      "[iter=13801]  loss = 0.2473\n",
      "[iter=13901]  loss = 0.2472\n",
      "[iter=14001]  loss = 0.2473\n",
      "[iter=14101]  loss = 0.2468\n",
      "[iter=14201]  loss = 0.2474\n",
      "[iter=14301]  loss = 0.2475\n",
      "[iter=14401]  loss = 0.2473\n",
      "[iter=14501]  loss = 0.2470\n",
      "[iter=14601]  loss = 0.2472\n",
      "[iter=14701]  loss = 0.2471\n",
      "[iter=14801]  loss = 0.2471\n",
      "[iter=14901]  loss = 0.2470\n",
      "[iter=15001]  loss = 0.2470\n",
      "[iter=15101]  loss = 0.2471\n",
      "[iter=15201]  loss = 0.2473\n",
      "[iter=15301]  loss = 0.2469\n",
      "[iter=15401]  loss = 0.2470\n",
      "[iter=15501]  loss = 0.2471\n",
      "[iter=15601]  loss = 0.2469\n",
      "[iter=15701]  loss = 0.2466\n",
      "[iter=15801]  loss = 0.2465\n",
      "[iter=15901]  loss = 0.2466\n",
      "[iter=16001]  loss = 0.2467\n",
      "[iter=16101]  loss = 0.2465\n",
      "[iter=16201]  loss = 0.2469\n",
      "[iter=16301]  loss = 0.2471\n",
      "[iter=16401]  loss = 0.2465\n",
      "[iter=16501]  loss = 0.2465\n",
      "[iter=16601]  loss = 0.2467\n",
      "[iter=16701]  loss = 0.2468\n",
      "[iter=16801]  loss = 0.2462\n",
      "[iter=16901]  loss = 0.2461\n",
      "[iter=17001]  loss = 0.2470\n",
      "[iter=17101]  loss = 0.2466\n",
      "[iter=17201]  loss = 0.2464\n",
      "[iter=17301]  loss = 0.2467\n",
      "[iter=17401]  loss = 0.2462\n",
      "[iter=17501]  loss = 0.2458\n",
      "[iter=17601]  loss = 0.2464\n",
      "[iter=17701]  loss = 0.2460\n",
      "[iter=17801]  loss = 0.2466\n",
      "[iter=17901]  loss = 0.2461\n",
      "[iter=18001]  loss = 0.2457\n",
      "[iter=18101]  loss = 0.2465\n",
      "[iter=18201]  loss = 0.2463\n",
      "[iter=18301]  loss = 0.2461\n",
      "[iter=18401]  loss = 0.2457\n",
      "[iter=18501]  loss = 0.2451\n",
      "[iter=18601]  loss = 0.2461\n",
      "[iter=18701]  loss = 0.2459\n",
      "[iter=18801]  loss = 0.2459\n",
      "[iter=18901]  loss = 0.2458\n",
      "[iter=19001]  loss = 0.2459\n",
      "[iter=19101]  loss = 0.2463\n",
      "[iter=19201]  loss = 0.2462\n",
      "[iter=19301]  loss = 0.2459\n",
      "[iter=19401]  loss = 0.2456\n",
      "[iter=19501]  loss = 0.2451\n",
      "[iter=19601]  loss = 0.2460\n",
      "[iter=19701]  loss = 0.2454\n",
      "[iter=19801]  loss = 0.2459\n",
      "[iter=19901]  loss = 0.2454\n",
      "[iter=20001]  loss = 0.2458\n",
      "[iter=20101]  loss = 0.2455\n",
      "[iter=20201]  loss = 0.2457\n",
      "[iter=20301]  loss = 0.2459\n",
      "[iter=20401]  loss = 0.2456\n",
      "[iter=20501]  loss = 0.2458\n",
      "[iter=20601]  loss = 0.2453\n",
      "[iter=20701]  loss = 0.2464\n",
      "[iter=20801]  loss = 0.2456\n",
      "[iter=20901]  loss = 0.2457\n",
      "[iter=21001]  loss = 0.2453\n",
      "[iter=21101]  loss = 0.2450\n",
      "[iter=21201]  loss = 0.2455\n",
      "[iter=21301]  loss = 0.2454\n",
      "[iter=21401]  loss = 0.2456\n",
      "[iter=21501]  loss = 0.2449\n",
      "[iter=21601]  loss = 0.2454\n",
      "[iter=21701]  loss = 0.2452\n",
      "[iter=21801]  loss = 0.2453\n",
      "[iter=21901]  loss = 0.2451\n",
      "[iter=22001]  loss = 0.2450\n",
      "[iter=22101]  loss = 0.2448\n",
      "[iter=22201]  loss = 0.2453\n",
      "[iter=22301]  loss = 0.2454\n",
      "[iter=22401]  loss = 0.2450\n",
      "[iter=22501]  loss = 0.2453\n",
      "[iter=22601]  loss = 0.2448\n",
      "[iter=22701]  loss = 0.2448\n",
      "[iter=22801]  loss = 0.2446\n",
      "[iter=22901]  loss = 0.2445\n",
      "[iter=23001]  loss = 0.2442\n",
      "[iter=23101]  loss = 0.2451\n",
      "[iter=23201]  loss = 0.2441\n",
      "[iter=23301]  loss = 0.2451\n",
      "[iter=23401]  loss = 0.2448\n",
      "[iter=23501]  loss = 0.2451\n",
      "[iter=23601]  loss = 0.2439\n",
      "[iter=23701]  loss = 0.2455\n",
      "[iter=23801]  loss = 0.2448\n",
      "[iter=23901]  loss = 0.2446\n",
      "[iter=24001]  loss = 0.2447\n",
      "[iter=24101]  loss = 0.2445\n",
      "[iter=24201]  loss = 0.2448\n",
      "[iter=24301]  loss = 0.2446\n",
      "[iter=24401]  loss = 0.2445\n",
      "[iter=24501]  loss = 0.2445\n",
      "[iter=24601]  loss = 0.2445\n",
      "[iter=24701]  loss = 0.2445\n",
      "[iter=24801]  loss = 0.2443\n",
      "[iter=24901]  loss = 0.2442\n",
      "[iter=25001]  loss = 0.2439\n",
      "[iter=25101]  loss = 0.2440\n",
      "[iter=25201]  loss = 0.2437\n",
      "[iter=25301]  loss = 0.2443\n",
      "[iter=25401]  loss = 0.2439\n",
      "[iter=25501]  loss = 0.2443\n",
      "[iter=25601]  loss = 0.2441\n",
      "[iter=25701]  loss = 0.2440\n",
      "[iter=25801]  loss = 0.2435\n",
      "[iter=25901]  loss = 0.2442\n",
      "[iter=26001]  loss = 0.2435\n",
      "[iter=26101]  loss = 0.2436\n",
      "[iter=26201]  loss = 0.2443\n",
      "[iter=26301]  loss = 0.2436\n",
      "[iter=26401]  loss = 0.2435\n",
      "[iter=26501]  loss = 0.2436\n",
      "[iter=26601]  loss = 0.2438\n",
      "[iter=26701]  loss = 0.2436\n",
      "[iter=26801]  loss = 0.2432\n",
      "[iter=26901]  loss = 0.2435\n",
      "[iter=27001]  loss = 0.2436\n",
      "[iter=27101]  loss = 0.2435\n",
      "[iter=27201]  loss = 0.2434\n",
      "[iter=27301]  loss = 0.2438\n",
      "[iter=27401]  loss = 0.2434\n",
      "[iter=27501]  loss = 0.2433\n",
      "[iter=27601]  loss = 0.2434\n",
      "[iter=27701]  loss = 0.2436\n",
      "[iter=27801]  loss = 0.2434\n",
      "[iter=27901]  loss = 0.2434\n",
      "[iter=28001]  loss = 0.2431\n",
      "[iter=28101]  loss = 0.2429\n",
      "[iter=28201]  loss = 0.2430\n",
      "[iter=28301]  loss = 0.2435\n",
      "[iter=28401]  loss = 0.2432\n",
      "[iter=28501]  loss = 0.2430\n",
      "[iter=28601]  loss = 0.2434\n",
      "[iter=28701]  loss = 0.2429\n",
      "[iter=28801]  loss = 0.2430\n",
      "[iter=28901]  loss = 0.2424\n",
      "[iter=29001]  loss = 0.2440\n",
      "[iter=29101]  loss = 0.2427\n",
      "[iter=29201]  loss = 0.2431\n",
      "[iter=29301]  loss = 0.2427\n",
      "[iter=29401]  loss = 0.2429\n",
      "[iter=29501]  loss = 0.2426\n",
      "[iter=29601]  loss = 0.2424\n",
      "[iter=29701]  loss = 0.2420\n",
      "[iter=29801]  loss = 0.2424\n",
      "[iter=29901]  loss = 0.2427\n",
      "[iter=30001]  loss = 0.2424\n",
      "[iter=30101]  loss = 0.2423\n",
      "[iter=30201]  loss = 0.2417\n",
      "[iter=30301]  loss = 0.2424\n",
      "[iter=30401]  loss = 0.2418\n",
      "[iter=30501]  loss = 0.2415\n",
      "[iter=30601]  loss = 0.2420\n",
      "[iter=30701]  loss = 0.2419\n",
      "[iter=30801]  loss = 0.2408\n",
      "[iter=30901]  loss = 0.2431\n",
      "[iter=31001]  loss = 0.2419\n",
      "[iter=31101]  loss = 0.2412\n",
      "[iter=31201]  loss = 0.2417\n",
      "[iter=31301]  loss = 0.2420\n",
      "[iter=31401]  loss = 0.2417\n",
      "[iter=31501]  loss = 0.2417\n",
      "[iter=31601]  loss = 0.2413\n",
      "[iter=31701]  loss = 0.2413\n",
      "[iter=31801]  loss = 0.2411\n",
      "[iter=31901]  loss = 0.2413\n",
      "[iter=32001]  loss = 0.2419\n",
      "[iter=32101]  loss = 0.2422\n",
      "[iter=32201]  loss = 0.2423\n",
      "[iter=32301]  loss = 0.2414\n",
      "[iter=32401]  loss = 0.2411\n",
      "[iter=32501]  loss = 0.2410\n",
      "[iter=32601]  loss = 0.2416\n",
      "[iter=32701]  loss = 0.2411\n",
      "[iter=32801]  loss = 0.2414\n",
      "[iter=32901]  loss = 0.2414\n",
      "[iter=33001]  loss = 0.2418\n",
      "[iter=33101]  loss = 0.2399\n",
      "[iter=33201]  loss = 0.2409\n",
      "[iter=33301]  loss = 0.2410\n",
      "[iter=33401]  loss = 0.2409\n",
      "[iter=33501]  loss = 0.2401\n",
      "[iter=33601]  loss = 0.2404\n",
      "[iter=33701]  loss = 0.2407\n",
      "[iter=33801]  loss = 0.2401\n",
      "[iter=33901]  loss = 0.2408\n",
      "[iter=34001]  loss = 0.2408\n",
      "[iter=34101]  loss = 0.2410\n",
      "[iter=34201]  loss = 0.2407\n",
      "[iter=34301]  loss = 0.2406\n",
      "[iter=34401]  loss = 0.2407\n",
      "[iter=34501]  loss = 0.2406\n",
      "[iter=34601]  loss = 0.2400\n",
      "[iter=34701]  loss = 0.2404\n",
      "[iter=34801]  loss = 0.2407\n",
      "[iter=34901]  loss = 0.2395\n",
      "[iter=35001]  loss = 0.2397\n",
      "[iter=35101]  loss = 0.2396\n",
      "[iter=35201]  loss = 0.2403\n",
      "[iter=35301]  loss = 0.2407\n",
      "[iter=35401]  loss = 0.2401\n",
      "[iter=35501]  loss = 0.2398\n",
      "[iter=35601]  loss = 0.2399\n",
      "[iter=35701]  loss = 0.2404\n",
      "[iter=35801]  loss = 0.2390\n",
      "[iter=35901]  loss = 0.2398\n",
      "[iter=36001]  loss = 0.2392\n",
      "[iter=36101]  loss = 0.2407\n",
      "[iter=36201]  loss = 0.2393\n",
      "[iter=36301]  loss = 0.2394\n",
      "[iter=36401]  loss = 0.2396\n",
      "[iter=36501]  loss = 0.2391\n",
      "[iter=36601]  loss = 0.2391\n",
      "[iter=36701]  loss = 0.2392\n",
      "[iter=36801]  loss = 0.2398\n",
      "[iter=36901]  loss = 0.2398\n",
      "[iter=37001]  loss = 0.2386\n",
      "[iter=37101]  loss = 0.2386\n",
      "[iter=37201]  loss = 0.2389\n",
      "[iter=37301]  loss = 0.2387\n",
      "[iter=37401]  loss = 0.2389\n",
      "[iter=37501]  loss = 0.2386\n",
      "[iter=37601]  loss = 0.2390\n",
      "[iter=37701]  loss = 0.2389\n",
      "[iter=37801]  loss = 0.2384\n",
      "[iter=37901]  loss = 0.2388\n",
      "[iter=38001]  loss = 0.2383\n",
      "[iter=38101]  loss = 0.2386\n",
      "[iter=38201]  loss = 0.2380\n",
      "[iter=38301]  loss = 0.2386\n",
      "[iter=38401]  loss = 0.2374\n",
      "[iter=38501]  loss = 0.2380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iter=38601]  loss = 0.2383\n",
      "[iter=38701]  loss = 0.2385\n",
      "[iter=38801]  loss = 0.2378\n",
      "[iter=38901]  loss = 0.2390\n",
      "[iter=39001]  loss = 0.2375\n",
      "[iter=39101]  loss = 0.2382\n",
      "[iter=39201]  loss = 0.2374\n",
      "[iter=39301]  loss = 0.2375\n",
      "[iter=39401]  loss = 0.2369\n",
      "[iter=39501]  loss = 0.2379\n",
      "[iter=39601]  loss = 0.2376\n",
      "[iter=39701]  loss = 0.2378\n",
      "[iter=39801]  loss = 0.2378\n",
      "[iter=39901]  loss = 0.2379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d73d93cfa0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss3 = cgp2.run_training_loop_multi_neuron_model(training_data2,0.9,0.99)\n",
    "plt.plot(loss3,label = \"Adam loss\")\n",
    "\n",
    "\n",
    "loss1 = cgp1.run_training_loop_multi_neuron_model(training_data1)\n",
    "plt.plot(loss1,label = \"SGD loss\")\n",
    "\n",
    "loss2 = cgp1.run_training_loop_multi_neuron_model(training_data1,0.9,True)\n",
    "plt.plot(loss2,label = \"SGD+ loss\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iterations in hundreds\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Multi neuron loss using different optimizers\")\n",
    "\n",
    "plt.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28622b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
